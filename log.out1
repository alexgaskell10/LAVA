\n\nTraining the victim model using config bin/config/ruletaker/rulereasoning_config_2021-12-12_17-38-38.jsonnet. \nOutputs will be saved to bin/runs/ruletaker/2021-12-12_17-38-38_roberta-large\n\n
python main.py ruletaker_train_original bin/config/ruletaker/rulereasoning_config_2021-12-12_17-38-38.jsonnet -s bin/runs/ruletaker/2021-12-12_17-38-38_roberta-large --include-package ruletaker.allennlp_models
wandb: Currently logged in as: alexgaskell (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.11.0
wandb: Syncing run comfy-meadow-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/alexgaskell/ruletaker
wandb: üöÄ View run at https://wandb.ai/alexgaskell/ruletaker/runs/icbhqe9m
wandb: Run data is saved locally in /vol/bitbucket/aeg19/re-re/wandb/run-20211212_173846-icbhqe9m
wandb: Run `wandb offline` to turn off syncing.

2021-12-12 17:38:51,198 - INFO - allennlp.common.params - random_seed = 13370
2021-12-12 17:38:51,198 - INFO - allennlp.common.params - numpy_seed = 1337
2021-12-12 17:38:51,198 - INFO - allennlp.common.params - pytorch_seed = 133
2021-12-12 17:38:51,912 - INFO - allennlp.common.checks - Pytorch version: 1.10.0+cu102
2021-12-12 17:38:51,914 - INFO - allennlp.common.params - type = default
2021-12-12 17:38:51,915 - INFO - allennlp.common.params - dataset_reader.type = rule_reasoning
2021-12-12 17:38:51,915 - INFO - allennlp.common.params - dataset_reader.pretrained_model = roberta-large
2021-12-12 17:38:51,915 - INFO - allennlp.common.params - dataset_reader.max_pieces = 384
2021-12-12 17:38:51,916 - INFO - allennlp.common.params - dataset_reader.syntax = rulebase
2021-12-12 17:38:51,916 - INFO - allennlp.common.params - dataset_reader.skip_id_regex = $none
2021-12-12 17:38:51,916 - INFO - allennlp.common.params - dataset_reader.scramble_context = False
2021-12-12 17:38:51,916 - INFO - allennlp.common.params - dataset_reader.use_context_full = False
2021-12-12 17:38:51,916 - INFO - allennlp.common.params - dataset_reader.sample = -1
2021-12-12 17:38:51,916 - INFO - allennlp.common.params - dataset_reader.max_instances = -1
2021-12-12 17:38:52,299 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748
2021-12-12 17:38:52,301 - INFO - transformers.configuration_utils - Model config RobertaConfig {
  "_num_labels": 2,
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_id": 2,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 514,
  "min_length": 0,
  "model_type": "roberta",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 1,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

2021-12-12 17:38:53,060 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
2021-12-12 17:38:53,061 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
2021-12-12 17:38:53,548 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748
2021-12-12 17:38:53,549 - INFO - transformers.configuration_utils - Model config RobertaConfig {
  "_num_labels": 2,
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_id": 2,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 514,
  "min_length": 0,
  "model_type": "roberta",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 1,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

2021-12-12 17:38:54,348 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
2021-12-12 17:38:54,349 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
2021-12-12 17:38:54,439 - INFO - allennlp.common.params - train_data_path = data/rule-reasoning-dataset-V2020.2.4/depth-5/train.jsonl
2021-12-12 17:38:54,440 - INFO - allennlp.common.params - vocabulary = None
2021-12-12 17:38:54,440 - INFO - allennlp.common.params - datasets_for_vocab_creation = None
2021-12-12 17:38:54,440 - INFO - allennlp.common.params - validation_dataset_reader = None
2021-12-12 17:38:54,440 - INFO - allennlp.common.params - validation_data_path = data/rule-reasoning-dataset-V2020.2.4/depth-5/dev.jsonl
2021-12-12 17:38:54,440 - INFO - allennlp.common.params - validation_data_loader = None
0it [00:00, ?it/s]2021-12-12 17:38:54,441 - INFO - allennlp.common.params - test_data_path = data/rule-reasoning-dataset-V2020.2.4/depth-5/test.jsonl
2021-12-12 17:38:54,441 - INFO - allennlp.common.params - evaluate_on_test = False
2021-12-12 17:38:54,441 - INFO - allennlp.training.util - Reading training data from data/rule-reasoning-dataset-V2020.2.4/depth-5/train.jsonl
2021-12-12 17:38:54,442 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - Reading instances from jsonl dataset at: data/rule-reasoning-dataset-V2020.2.4/depth-5/train.jsonl
2021-12-12 17:38:54,442 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNoneg-D5-743', 'context': 'If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.', 'meta': {'sentenceScramble': [12, 9, 15, 10, 11, 13, 6, 3, 1, 17, 2, 8, 14, 5, 7, 4, 16]}, 'questions': [{'id': 'AttNoneg-D5-743-1', 'text': 'Gary is green.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNoneg-D5-743-2', 'text': 'Dave is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNoneg-D5-743-3', 'text': 'Dave is smart.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNoneg-D5-743-4', 'text': 'Dave is not smart.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNoneg-D5-743-5', 'text': 'Gary is big.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNoneg-D5-743-6', 'text': 'Gary is not big.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNoneg-D5-743-7', 'text': 'Dave is white.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNoneg-D5-743-8', 'text': 'Dave is not white.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNoneg-D5-743-9', 'text': 'Dave is green.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNoneg-D5-743-10', 'text': 'Dave is not green.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNoneg-D5-743-11', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNoneg-D5-743-12', 'text': 'Dave is not round.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNoneg-D5-743-13', 'text': 'Harry is not round.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNoneg-D5-743-14', 'text': 'Harry is big.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNoneg-D5-743-15', 'text': 'Harry is not white.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNoneg-D5-743-16', 'text': 'Harry is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:38:54,444 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Gary, ƒ†is, ƒ†green, ., </s>, </s>, ƒ†C, :, ƒ†If, ƒ†something, ƒ†is, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†Gary, ƒ†is, ƒ†young, ., ƒ†If, ƒ†Dave, ƒ†is, ƒ†white, ƒ†and, ƒ†Dave, ƒ†is, ƒ†young, ƒ†then, ƒ†Dave, ƒ†is, ƒ†green, ., ƒ†Harry, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†round, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†white, ., ƒ†Gary, ƒ†is, ƒ†green, ., ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†green, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†Gary, ƒ†is, ƒ†round, ., ƒ†All, ƒ†white, ƒ†things, ƒ†are, ƒ†big, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†Gary, ƒ†is, ƒ†kind, ., ƒ†Dave, ƒ†is, ƒ†big, ., ƒ†All, ƒ†kind, ,, ƒ†young, ƒ†things, ƒ†are, ƒ†white, ., </s>]
2021-12-12 17:38:54,444 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.
2021-12-12 17:38:54,444 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 1
2021-12-12 17:38:54,445 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNoneg-D5-743', 'context': 'If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.', 'meta': {'sentenceScramble': [12, 9, 15, 10, 11, 13, 6, 3, 1, 17, 2, 8, 14, 5, 7, 4, 16]}, 'questions': [{'id': 'AttNoneg-D5-743-1', 'text': 'Gary is green.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNoneg-D5-743-2', 'text': 'Dave is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNoneg-D5-743-3', 'text': 'Dave is smart.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNoneg-D5-743-4', 'text': 'Dave is not smart.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNoneg-D5-743-5', 'text': 'Gary is big.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNoneg-D5-743-6', 'text': 'Gary is not big.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNoneg-D5-743-7', 'text': 'Dave is white.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNoneg-D5-743-8', 'text': 'Dave is not white.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNoneg-D5-743-9', 'text': 'Dave is green.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNoneg-D5-743-10', 'text': 'Dave is not green.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNoneg-D5-743-11', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNoneg-D5-743-12', 'text': 'Dave is not round.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNoneg-D5-743-13', 'text': 'Harry is not round.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNoneg-D5-743-14', 'text': 'Harry is big.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNoneg-D5-743-15', 'text': 'Harry is not white.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNoneg-D5-743-16', 'text': 'Harry is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:38:54,447 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Dave, ƒ†is, ƒ†not, ƒ†young, ., </s>, </s>, ƒ†C, :, ƒ†If, ƒ†something, ƒ†is, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†Gary, ƒ†is, ƒ†young, ., ƒ†If, ƒ†Dave, ƒ†is, ƒ†white, ƒ†and, ƒ†Dave, ƒ†is, ƒ†young, ƒ†then, ƒ†Dave, ƒ†is, ƒ†green, ., ƒ†Harry, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†round, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†white, ., ƒ†Gary, ƒ†is, ƒ†green, ., ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†green, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†Gary, ƒ†is, ƒ†round, ., ƒ†All, ƒ†white, ƒ†things, ƒ†are, ƒ†big, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†Gary, ƒ†is, ƒ†kind, ., ƒ†Dave, ƒ†is, ƒ†big, ., ƒ†All, ƒ†kind, ,, ƒ†young, ƒ†things, ƒ†are, ƒ†white, ., </s>]
2021-12-12 17:38:54,447 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.
2021-12-12 17:38:54,447 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 0
2021-12-12 17:38:54,447 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNoneg-D5-743', 'context': 'If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.', 'meta': {'sentenceScramble': [12, 9, 15, 10, 11, 13, 6, 3, 1, 17, 2, 8, 14, 5, 7, 4, 16]}, 'questions': [{'id': 'AttNoneg-D5-743-1', 'text': 'Gary is green.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNoneg-D5-743-2', 'text': 'Dave is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNoneg-D5-743-3', 'text': 'Dave is smart.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNoneg-D5-743-4', 'text': 'Dave is not smart.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNoneg-D5-743-5', 'text': 'Gary is big.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNoneg-D5-743-6', 'text': 'Gary is not big.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNoneg-D5-743-7', 'text': 'Dave is white.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNoneg-D5-743-8', 'text': 'Dave is not white.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNoneg-D5-743-9', 'text': 'Dave is green.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNoneg-D5-743-10', 'text': 'Dave is not green.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNoneg-D5-743-11', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNoneg-D5-743-12', 'text': 'Dave is not round.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNoneg-D5-743-13', 'text': 'Harry is not round.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNoneg-D5-743-14', 'text': 'Harry is big.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNoneg-D5-743-15', 'text': 'Harry is not white.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNoneg-D5-743-16', 'text': 'Harry is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:38:54,448 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Dave, ƒ†is, ƒ†smart, ., </s>, </s>, ƒ†C, :, ƒ†If, ƒ†something, ƒ†is, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†Gary, ƒ†is, ƒ†young, ., ƒ†If, ƒ†Dave, ƒ†is, ƒ†white, ƒ†and, ƒ†Dave, ƒ†is, ƒ†young, ƒ†then, ƒ†Dave, ƒ†is, ƒ†green, ., ƒ†Harry, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†round, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†white, ., ƒ†Gary, ƒ†is, ƒ†green, ., ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†green, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†Gary, ƒ†is, ƒ†round, ., ƒ†All, ƒ†white, ƒ†things, ƒ†are, ƒ†big, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†Gary, ƒ†is, ƒ†kind, ., ƒ†Dave, ƒ†is, ƒ†big, ., ƒ†All, ƒ†kind, ,, ƒ†young, ƒ†things, ƒ†are, ƒ†white, ., </s>]
2021-12-12 17:38:54,448 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.
2021-12-12 17:38:54,448 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 1
2021-12-12 17:38:54,448 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNoneg-D5-743', 'context': 'If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.', 'meta': {'sentenceScramble': [12, 9, 15, 10, 11, 13, 6, 3, 1, 17, 2, 8, 14, 5, 7, 4, 16]}, 'questions': [{'id': 'AttNoneg-D5-743-1', 'text': 'Gary is green.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNoneg-D5-743-2', 'text': 'Dave is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNoneg-D5-743-3', 'text': 'Dave is smart.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNoneg-D5-743-4', 'text': 'Dave is not smart.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNoneg-D5-743-5', 'text': 'Gary is big.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNoneg-D5-743-6', 'text': 'Gary is not big.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNoneg-D5-743-7', 'text': 'Dave is white.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNoneg-D5-743-8', 'text': 'Dave is not white.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNoneg-D5-743-9', 'text': 'Dave is green.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNoneg-D5-743-10', 'text': 'Dave is not green.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNoneg-D5-743-11', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNoneg-D5-743-12', 'text': 'Dave is not round.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNoneg-D5-743-13', 'text': 'Harry is not round.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNoneg-D5-743-14', 'text': 'Harry is big.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNoneg-D5-743-15', 'text': 'Harry is not white.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNoneg-D5-743-16', 'text': 'Harry is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:38:54,449 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Dave, ƒ†is, ƒ†not, ƒ†smart, ., </s>, </s>, ƒ†C, :, ƒ†If, ƒ†something, ƒ†is, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†Gary, ƒ†is, ƒ†young, ., ƒ†If, ƒ†Dave, ƒ†is, ƒ†white, ƒ†and, ƒ†Dave, ƒ†is, ƒ†young, ƒ†then, ƒ†Dave, ƒ†is, ƒ†green, ., ƒ†Harry, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†round, ., ƒ†All, ƒ†smart, ,, ƒ†green, ƒ†things, ƒ†are, ƒ†white, ., ƒ†Gary, ƒ†is, ƒ†green, ., ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†green, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†Gary, ƒ†is, ƒ†round, ., ƒ†All, ƒ†white, ƒ†things, ƒ†are, ƒ†big, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†Gary, ƒ†is, ƒ†kind, ., ƒ†Dave, ƒ†is, ƒ†big, ., ƒ†All, ƒ†kind, ,, ƒ†young, ƒ†things, ƒ†are, ƒ†white, ., </s>]
2021-12-12 17:38:54,450 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = If something is smart then it is kind. Gary is young. If Dave is white and Dave is young then Dave is green. Harry is smart. All smart, green things are round. All smart, green things are white. Gary is green. Anne is young. Anne is green. All young things are smart. Anne is smart. Gary is round. All white things are big. Dave is young. Gary is kind. Dave is big. All kind, young things are white.
2021-12-12 17:38:54,450 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 0
20it [00:00, 173.09it/s]106it [00:00, 549.56it/s]209it [00:00, 757.62it/s]287it [00:00, 493.99it/s]403it [00:00, 668.15it/s]483it [00:00, 596.42it/s]575it [00:00, 675.14it/s]680it [00:01, 772.30it/s]773it [00:01, 812.76it/s]861it [00:01, 609.05it/s]954it [00:01, 681.09it/s]1062it [00:01, 776.78it/s]1155it [00:01, 815.91it/s]1273it [00:01, 912.88it/s]1371it [00:01, 918.93it/s]1503it [00:01, 1030.02it/s]1610it [00:02, 691.72it/s] 1709it [00:02, 755.40it/s]1823it [00:02, 844.46it/s]1941it [00:02, 927.49it/s]2045it [00:02, 956.28it/s]2149it [00:02, 968.87it/s]2252it [00:02, 971.74it/s]2355it [00:02, 986.70it/s]2463it [00:03, 618.48it/s]2552it [00:03, 671.27it/s]2642it [00:03, 720.96it/s]2758it [00:03, 825.64it/s]2877it [00:03, 916.88it/s]2979it [00:03, 935.41it/s]3082it [00:03, 959.70it/s]3184it [00:03, 952.30it/s]3294it [00:04, 990.17it/s]3409it [00:04, 1034.23it/s]3541it [00:04, 1116.63it/s]3655it [00:04, 670.98it/s] 3762it [00:04, 749.79it/s]3861it [00:04, 802.96it/s]3970it [00:04, 871.24it/s]4070it [00:05, 901.80it/s]4170it [00:05, 923.49it/s]4276it [00:05, 959.98it/s]4388it [00:05, 1004.48it/s]4496it [00:05, 1025.55it/s]4602it [00:05, 1011.95it/s]4710it [00:05, 1030.66it/s]4815it [00:05, 1029.30it/s]4919it [00:05, 988.26it/s] 5024it [00:05, 1005.22it/s]5126it [00:06, 571.01it/s] 5236it [00:06, 670.94it/s]5339it [00:06, 745.80it/s]5433it [00:06, 790.21it/s]5542it [00:06, 858.97it/s]5658it [00:06, 936.57it/s]5769it [00:06, 982.40it/s]5876it [00:07, 1004.32it/s]5988it [00:07, 1036.99it/s]6098it [00:07, 1054.44it/s]6211it [00:07, 1075.61it/s]6331it [00:07, 1109.83it/s]6444it [00:07, 1029.22it/s]6549it [00:07, 1027.03it/s]6663it [00:07, 1059.09it/s]6772it [00:07, 1066.49it/s]6884it [00:07, 1081.23it/s]6993it [00:08, 502.62it/s] 7086it [00:08, 571.99it/s]7203it [00:08, 683.60it/s]7307it [00:08, 758.45it/s]7415it [00:08, 831.04it/s]7526it [00:08, 897.81it/s]7630it [00:09, 912.58it/s]7748it [00:09, 983.02it/s]7854it [00:09, 1003.48it/s]7960it [00:09, 1001.73it/s]8072it [00:09, 1034.38it/s]8201it [00:09, 1106.91it/s]8319it [00:09, 1124.64it/s]8434it [00:09, 1102.77it/s]8546it [00:09, 1100.30it/s]8666it [00:09, 1126.35it/s]8785it [00:10, 1143.35it/s]8900it [00:10, 1098.28it/s]9011it [00:10, 926.28it/s] 9111it [00:10, 943.29it/s]9209it [00:10, 908.30it/s]9303it [00:11, 455.94it/s]9412it [00:11, 556.66it/s]9532it [00:11, 675.27it/s]9642it [00:11, 763.35it/s]9741it [00:11, 779.07it/s]9840it [00:11, 827.96it/s]9954it [00:11, 905.77it/s]10061it [00:11, 948.59it/s]10164it [00:11, 964.70it/s]10272it [00:11, 996.27it/s]10390it [00:12, 1046.13it/s]10498it [00:12, 995.14it/s] 10604it [00:12, 1012.56it/s]10716it [00:12, 1042.71it/s]10822it [00:12, 1033.46it/s]10927it [00:12, 1005.01it/s]11029it [00:12, 949.60it/s] 11147it [00:12, 1013.11it/s]11269it [00:12, 1070.04it/s]11378it [00:13, 1049.63it/s]11484it [00:13, 1033.44it/s]11588it [00:13, 998.15it/s] 11689it [00:13, 938.33it/s]11820it [00:13, 1037.67it/s]11926it [00:13, 921.41it/s] 12040it [00:13, 977.82it/s]12141it [00:14, 461.29it/s]12242it [00:14, 545.64it/s]12342it [00:14, 626.95it/s]12451it [00:14, 720.89it/s]12559it [00:14, 801.35it/s]12661it [00:14, 853.53it/s]12773it [00:14, 921.75it/s]12877it [00:14, 915.30it/s]12986it [00:15, 960.96it/s]13098it [00:15, 1003.96it/s]13204it [00:15, 1019.59it/s]13312it [00:15, 1036.14it/s]13427it [00:15, 1068.85it/s]13536it [00:15, 1029.94it/s]13642it [00:15, 1037.60it/s]13773it [00:15, 1114.45it/s]13891it [00:15, 1133.12it/s]14013it [00:16, 1157.15it/s]14130it [00:16, 1135.71it/s]14245it [00:16, 1125.41it/s]14358it [00:16, 1108.62it/s]14470it [00:16, 1084.60it/s]14589it [00:16, 1113.57it/s]14710it [00:16, 1138.80it/s]14825it [00:16, 1086.14it/s]14935it [00:16, 1055.41it/s]15042it [00:16, 1018.78it/s]15145it [00:17, 1014.25it/s]15254it [00:17, 1034.10it/s]15359it [00:17, 1038.28it/s]15467it [00:17, 1047.89it/s]15575it [00:17, 1056.68it/s]15681it [00:18, 377.28it/s] 15765it [00:18, 437.59it/s]15855it [00:18, 509.86it/s]15959it [00:18, 607.01it/s]16075it [00:18, 721.64it/s]16184it [00:18, 805.80it/s]16309it [00:18, 914.68it/s]16417it [00:18, 908.94it/s]16536it [00:19, 980.85it/s]16644it [00:19, 981.61it/s]16756it [00:19, 1017.88it/s]16871it [00:19, 1052.67it/s]16999it [00:19, 1116.34it/s]17139it [00:19, 1197.94it/s]17262it [00:19, 1147.60it/s]17379it [00:19, 1153.33it/s]17507it [00:19, 1189.63it/s]17635it [00:19, 1215.49it/s]17758it [00:20, 1198.32it/s]17881it [00:20, 1204.50it/s]18002it [00:20, 1135.69it/s]18119it [00:20, 1142.52it/s]18248it [00:20, 1183.58it/s]18368it [00:20, 1143.70it/s]18484it [00:20, 1090.82it/s]18616it [00:20, 1153.63it/s]18733it [00:20, 1122.73it/s]18865it [00:21, 1177.19it/s]19012it [00:21, 1259.12it/s]19139it [00:21, 1201.45it/s]19261it [00:21, 1189.33it/s]19381it [00:21, 1175.85it/s]19500it [00:21, 1162.87it/s]19617it [00:21, 1158.19it/s]19738it [00:21, 1172.46it/s]19856it [00:21, 1086.52it/s]19966it [00:22, 1042.88it/s]20076it [00:22, 1058.33it/s]20183it [00:22, 1049.08it/s]20289it [00:22, 363.55it/s] 20379it [00:23, 429.38it/s]20474it [00:23, 506.79it/s]20578it [00:23, 600.51it/s]20695it [00:23, 714.29it/s]20809it [00:23, 809.01it/s]20936it [00:23, 918.01it/s]21046it [00:23, 932.46it/s]21159it [00:23, 981.38it/s]21267it [00:23, 1001.53it/s]21389it [00:23, 1061.71it/s]21501it [00:24, 1073.72it/s]21613it [00:24, 1061.45it/s]21723it [00:24, 1070.23it/s]21832it [00:24, 1056.26it/s]21948it [00:24, 1085.96it/s]22060it [00:24, 1094.15it/s]22176it [00:24, 1112.50it/s]22288it [00:24, 1098.63it/s]22408it [00:24, 1126.71it/s]22522it [00:25, 1057.94it/s]22632it [00:25, 1068.61it/s]22762it [00:25, 1132.68it/s]22877it [00:25, 1049.03it/s]22984it [00:25, 1038.42it/s]23101it [00:25, 1074.30it/s]23210it [00:25, 1039.46it/s]23315it [00:25, 1040.96it/s]23450it [00:25, 1127.99it/s]23564it [00:26, 1069.73it/s]23689it [00:26, 1119.08it/s]23802it [00:26, 1096.10it/s]23939it [00:26, 1172.04it/s]24058it [00:26, 1140.49it/s]24178it [00:26, 1157.26it/s]24295it [00:26, 1121.33it/s]24408it [00:26, 1106.76it/s]24520it [00:26, 1076.57it/s]24647it [00:26, 1129.48it/s]24761it [00:27, 1073.77it/s]24870it [00:27, 1042.23it/s]24981it [00:27, 1060.04it/s]25088it [00:27, 1037.17it/s]25195it [00:27, 1045.28it/s]25318it [00:27, 1097.31it/s]25437it [00:27, 1122.99it/s]25573it [00:27, 1192.44it/s]25693it [00:27, 1191.67it/s]25813it [00:28, 1165.89it/s]25930it [00:28, 354.87it/s] 26043it [00:28, 442.02it/s]26150it [00:29, 528.73it/s]26261it [00:29, 623.75it/s]26366it [00:29, 704.62it/s]26474it [00:29, 784.57it/s]26609it [00:29, 915.77it/s]26724it [00:29, 973.31it/s]26846it [00:29, 1037.49it/s]26974it [00:29, 1102.16it/s]27094it [00:29, 1103.10it/s]27214it [00:30, 1127.30it/s]27332it [00:30, 1102.37it/s]27446it [00:30, 1082.81it/s]27557it [00:30, 1061.53it/s]27666it [00:30, 1067.76it/s]27789it [00:30, 1110.86it/s]27902it [00:30, 1105.26it/s]28014it [00:30, 1052.63it/s]28121it [00:30, 1025.87it/s]28226it [00:30, 1032.44it/s]28330it [00:31, 982.98it/s] 28453it [00:31, 1050.25it/s]28559it [00:31, 1041.26it/s]28668it [00:31, 1052.85it/s]28793it [00:31, 1107.99it/s]28905it [00:31, 1106.58it/s]29020it [00:31, 1118.75it/s]29133it [00:31, 1102.85it/s]29244it [00:31, 1087.05it/s]29361it [00:32, 1108.45it/s]29473it [00:32, 1085.45it/s]29587it [00:32, 1099.72it/s]29706it [00:32, 1125.00it/s]29819it [00:32, 1058.53it/s]29926it [00:32, 1031.50it/s]30030it [00:32, 1030.60it/s]30134it [00:32, 1024.80it/s]30237it [00:32, 1020.11it/s]30340it [00:32, 986.83it/s] 30444it [00:33, 1000.66it/s]30545it [00:33, 997.59it/s] 30664it [00:33, 1052.36it/s]30781it [00:33, 1082.35it/s]30890it [00:33, 1073.52it/s]30998it [00:33, 1056.32it/s]31107it [00:33, 1063.59it/s]31214it [00:33, 1044.42it/s]31319it [00:33, 1039.76it/s]31431it [00:33, 1062.71it/s]31538it [00:34, 1064.61it/s]31645it [00:34, 1047.67it/s]31750it [00:34, 1037.09it/s]31854it [00:34, 1025.27it/s]31957it [00:34, 1017.91it/s]32074it [00:34, 1060.20it/s]32190it [00:34, 1086.66it/s]32299it [00:34, 1007.99it/s]32405it [00:34, 1022.02it/s]32520it [00:35, 1056.34it/s]32627it [00:35, 1016.14it/s]32740it [00:35, 1048.16it/s]32846it [00:36, 265.47it/s] 32968it [00:36, 355.37it/s]33059it [00:36, 419.45it/s]33149it [00:36, 485.44it/s]33256it [00:36, 584.46it/s]33372it [00:36, 696.44it/s]33483it [00:36, 785.91it/s]33589it [00:37, 850.41it/s]33695it [00:37, 899.85it/s]33800it [00:37, 921.99it/s]33903it [00:37, 942.20it/s]34009it [00:37, 973.04it/s]34112it [00:37, 981.16it/s]34219it [00:37, 1003.81it/s]34323it [00:37, 1013.57it/s]34427it [00:37, 994.69it/s] 34559it [00:38, 1087.73it/s]34670it [00:38, 1069.99it/s]34780it [00:38, 1076.33it/s]34889it [00:38, 1059.74it/s]35015it [00:38, 1117.36it/s]35128it [00:38, 1092.42it/s]35248it [00:38, 1123.58it/s]35369it [00:38, 1145.12it/s]35496it [00:38, 1180.73it/s]35615it [00:38, 1153.15it/s]35736it [00:39, 1168.36it/s]35854it [00:39, 1073.91it/s]35963it [00:39, 1061.48it/s]36071it [00:39, 1064.57it/s]36201it [00:39, 1130.15it/s]36327it [00:39, 1165.74it/s]36445it [00:39, 1167.98it/s]36563it [00:39, 1151.09it/s]36679it [00:39, 1134.04it/s]36793it [00:40, 1026.33it/s]36902it [00:40, 1041.41it/s]37008it [00:40, 1038.95it/s]37113it [00:40, 1021.90it/s]37216it [00:40, 997.86it/s] 37328it [00:40, 1031.20it/s]37441it [00:40, 1057.55it/s]37548it [00:40, 1055.50it/s]37673it [00:40, 1110.98it/s]37785it [00:40, 1061.89it/s]37894it [00:41, 1067.91it/s]38029it [00:41, 1149.57it/s]38145it [00:41, 1093.54it/s]38256it [00:41, 1085.26it/s]38366it [00:41, 1032.53it/s]38471it [00:41, 1034.31it/s]38593it [00:41, 1086.02it/s]38703it [00:41, 1044.36it/s]38817it [00:41, 1068.93it/s]38925it [00:42, 1051.94it/s]39041it [00:42, 1080.12it/s]39150it [00:42, 1062.88it/s]39257it [00:42, 1060.23it/s]39366it [00:42, 1068.53it/s]39474it [00:42, 1066.36it/s]39581it [00:42, 1063.09it/s]39693it [00:42, 1078.28it/s]39801it [00:42, 1046.58it/s]39906it [00:42, 1021.41it/s]40009it [00:43, 1023.29it/s]40116it [00:43, 1035.09it/s]40220it [00:43, 1010.84it/s]40335it [00:43, 1049.95it/s]40451it [00:43, 1078.70it/s]40586it [00:43, 1157.60it/s]40703it [00:43, 1120.48it/s]40816it [00:43, 1077.08it/s]40925it [00:43, 1066.33it/s]41034it [00:44, 1072.97it/s]41142it [00:44, 1061.45it/s]41265it [00:44, 1108.22it/s]41380it [00:44, 1120.32it/s]41493it [00:45, 223.97it/s] 41592it [00:45, 283.11it/s]41716it [00:45, 378.28it/s]41832it [00:46, 475.40it/s]41954it [00:46, 587.08it/s]42063it [00:46, 663.10it/s]42180it [00:46, 762.80it/s]42298it [00:46, 854.12it/s]42410it [00:46, 893.38it/s]42530it [00:46, 970.00it/s]42643it [00:46, 1011.30it/s]42755it [00:46, 1040.01it/s]42867it [00:47, 1028.84it/s]42976it [00:47, 1042.63it/s]43098it [00:47, 1091.42it/s]43211it [00:47, 1062.54it/s]43321it [00:47, 1072.11it/s]43430it [00:47, 1060.46it/s]43538it [00:47, 1047.78it/s]43644it [00:47, 1026.37it/s]43748it [00:47, 991.48it/s] 43848it [00:47, 992.53it/s]43973it [00:48, 1064.14it/s]44094it [00:48, 1104.66it/s]44205it [00:48, 1101.43it/s]44319it [00:48, 1110.94it/s]44434it [00:48, 1119.52it/s]44547it [00:48, 1091.19it/s]44664it [00:48, 1112.61it/s]44777it [00:48, 1117.25it/s]44889it [00:48, 1110.30it/s]45002it [00:48, 1114.81it/s]45114it [00:49, 1113.95it/s]45240it [00:49, 1154.31it/s]45356it [00:49, 1137.90it/s]45476it [00:49, 1154.86it/s]45615it [00:49, 1222.99it/s]45738it [00:49, 1183.34it/s]45857it [00:49, 1153.57it/s]45973it [00:49, 1123.09it/s]46086it [00:49, 1118.68it/s]46204it [00:50, 1136.09it/s]46319it [00:50, 1139.34it/s]46434it [00:50, 1093.85it/s]46544it [00:50, 1080.82it/s]46653it [00:50, 1079.58it/s]46765it [00:50, 1089.04it/s]46884it [00:50, 1117.90it/s]46996it [00:50, 1117.68it/s]47108it [00:50, 1108.45it/s]47227it [00:50, 1132.21it/s]47352it [00:51, 1165.86it/s]47469it [00:51, 1102.13it/s]47580it [00:51, 1088.19it/s]47698it [00:51, 1111.95it/s]47810it [00:51, 1091.76it/s]47924it [00:51, 1105.50it/s]48035it [00:51, 1066.50it/s]48145it [00:51, 1074.19it/s]48253it [00:51, 1059.68it/s]48377it [00:52, 1111.93it/s]48505it [00:52, 1158.86it/s]48628it [00:52, 1177.35it/s]48746it [00:52, 1120.95it/s]48881it [00:52, 1184.86it/s]49001it [00:52, 1118.07it/s]49114it [00:52, 1091.14it/s]49224it [00:52, 1070.77it/s]49344it [00:52, 1107.04it/s]49467it [00:52, 1141.72it/s]49582it [00:53, 1122.83it/s]49695it [00:53, 1085.09it/s]49807it [00:53, 1094.67it/s]49917it [00:53, 1039.37it/s]50032it [00:53, 1068.20it/s]50140it [00:53, 1050.86it/s]50246it [00:53, 1046.87it/s]50352it [00:53, 1050.12it/s]50458it [00:53, 1021.62it/s]50568it [00:54, 1042.19it/s]50673it [00:54, 1033.20it/s]50789it [00:54, 1069.85it/s]50902it [00:54, 1085.23it/s]51011it [00:54, 1030.52it/s]51126it [00:54, 1062.84it/s]51260it [00:54, 1140.61it/s]51379it [00:54, 1153.53it/s]51495it [00:54, 1112.09it/s]51624it [00:54, 1160.11it/s]51741it [00:55, 1147.80it/s]51857it [00:55, 1097.15it/s]51982it [00:55, 1138.54it/s]52097it [00:55, 1132.35it/s]52211it [00:55, 1104.05it/s]52340it [00:55, 1157.27it/s]52457it [00:55, 1108.35it/s]52569it [00:57, 189.82it/s] 52686it [00:57, 253.28it/s]52781it [00:57, 312.01it/s]52908it [00:57, 415.32it/s]53026it [00:57, 517.17it/s]53137it [00:58, 611.44it/s]53245it [00:58, 676.43it/s]53349it [00:58, 749.73it/s]53456it [00:58, 821.96it/s]53570it [00:58, 898.98it/s]53693it [00:58, 983.25it/s]53815it [00:58, 1046.84it/s]53930it [00:58, 1018.66it/s]54040it [00:58, 1033.66it/s]54149it [00:58, 1041.23it/s]54262it [00:59, 1065.97it/s]54372it [00:59, 1051.64it/s]54488it [00:59, 1081.23it/s]54599it [00:59, 1087.69it/s]54726it [00:59, 1139.67it/s]54841it [00:59, 1091.30it/s]54970it [00:59, 1146.34it/s]55086it [00:59, 1123.31it/s]55209it [00:59, 1153.85it/s]55325it [01:00, 1106.51it/s]55437it [01:00, 1108.16it/s]55552it [01:00, 1117.75it/s]55669it [01:00, 1132.23it/s]55783it [01:00, 1087.40it/s]55893it [01:00, 1077.32it/s]56015it [01:00, 1116.79it/s]56128it [01:00, 1105.34it/s]56239it [01:00, 1078.72it/s]56363it [01:00, 1123.40it/s]56476it [01:01, 1045.47it/s]56597it [01:01, 1090.00it/s]56721it [01:01, 1129.67it/s]56836it [01:01, 1134.32it/s]56951it [01:01, 1124.40it/s]57071it [01:01, 1144.21it/s]57186it [01:01, 1103.65it/s]57312it [01:01, 1146.48it/s]57441it [01:01, 1187.12it/s]57561it [01:02, 1185.99it/s]57697it [01:02, 1236.91it/s]57822it [01:02, 1181.43it/s]57941it [01:02, 1143.06it/s]58073it [01:02, 1192.82it/s]58194it [01:02, 1140.47it/s]58309it [01:02, 1112.27it/s]58421it [01:02, 1053.35it/s]58546it [01:02, 1107.10it/s]58660it [01:02, 1116.28it/s]58780it [01:03, 1139.04it/s]58925it [01:03, 1228.38it/s]59049it [01:03, 1169.32it/s]59167it [01:03, 1170.52it/s]59285it [01:03, 1155.88it/s]59407it [01:03, 1173.35it/s]59532it [01:03, 1195.15it/s]59652it [01:03, 1131.82it/s]59767it [01:03, 1101.00it/s]59889it [01:04, 1132.37it/s]60004it [01:04, 1134.25it/s]60118it [01:04, 1072.99it/s]60227it [01:04, 1046.71it/s]60333it [01:04, 1018.47it/s]60436it [01:04, 985.75it/s] 60535it [01:04, 967.31it/s]60661it [01:04, 1049.22it/s]60769it [01:04, 1057.62it/s]60876it [01:05, 1041.97it/s]60981it [01:05, 1041.99it/s]61097it [01:05, 1075.14it/s]61207it [01:05, 1080.90it/s]61332it [01:05, 1130.55it/s]61446it [01:05, 1067.53it/s]61554it [01:05, 1060.66it/s]61661it [01:05, 1053.49it/s]61796it [01:05, 1138.63it/s]61911it [01:05, 1133.61it/s]62031it [01:06, 1151.03it/s]62147it [01:06, 1126.79it/s]62260it [01:06, 1085.55it/s]62372it [01:06, 1094.32it/s]62483it [01:06, 1096.48it/s]62593it [01:06, 1049.99it/s]62699it [01:06, 1028.44it/s]62821it [01:06, 1081.03it/s]62931it [01:06, 1083.92it/s]63045it [01:06, 1096.63it/s]63155it [01:07, 1082.69it/s]63264it [01:07, 1042.26it/s]63372it [01:07, 1052.44it/s]63491it [01:07, 1092.09it/s]63601it [01:07, 1063.47it/s]63721it [01:07, 1101.05it/s]63833it [01:07, 1105.14it/s]63951it [01:07, 1125.78it/s]64064it [01:07, 1062.87it/s]64181it [01:08, 1089.92it/s]64291it [01:08, 1051.14it/s]64407it [01:08, 1078.39it/s]64524it [01:08, 1103.45it/s]64635it [01:08, 1105.01it/s]64746it [01:08, 1093.77it/s]64856it [01:08, 1072.56it/s]64966it [01:08, 1079.46it/s]65075it [01:08, 1053.26it/s]65181it [01:08, 1016.95it/s]65299it [01:09, 1062.77it/s]65411it [01:09, 1077.85it/s]65520it [01:09, 1075.54it/s]65628it [01:09, 1009.07it/s]65733it [01:09, 1019.61it/s]65855it [01:09, 1076.29it/s]65964it [01:09, 1018.82it/s]66089it [01:09, 1082.14it/s]66199it [01:11, 163.60it/s] 66312it [01:12, 219.55it/s]66412it [01:12, 279.20it/s]66532it [01:12, 369.81it/s]66654it [01:12, 474.36it/s]66761it [01:12, 561.28it/s]66890it [01:12, 688.36it/s]67003it [01:12, 772.10it/s]67152it [01:12, 930.72it/s]67275it [01:12, 973.75it/s]67394it [01:12, 1015.08it/s]67512it [01:13, 1005.72it/s]67626it [01:13, 1039.21it/s]67739it [01:13, 1048.16it/s]67850it [01:13, 1042.83it/s]67959it [01:13, 1043.11it/s]68067it [01:13, 1012.22it/s]68174it [01:13, 1025.82it/s]68279it [01:13, 1001.93it/s]68425it [01:13, 1130.27it/s]68540it [01:14, 1083.46it/s]68650it [01:14, 1065.27it/s]68758it [01:14, 1043.98it/s]68871it [01:14, 1066.30it/s]68979it [01:14, 1042.37it/s]69093it [01:14, 1068.08it/s]69201it [01:14, 1057.37it/s]69317it [01:14, 1086.65it/s]69428it [01:14, 1093.21it/s]69549it [01:14, 1126.37it/s]69692it [01:15, 1213.58it/s]69762it [01:15, 928.58it/s] 2021-12-12 17:40:09,570 - INFO - allennlp.training.util - Reading validation data from data/rule-reasoning-dataset-V2020.2.4/depth-5/dev.jsonl

0it [00:00, ?it/s]2021-12-12 17:40:09,571 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - Reading instances from jsonl dataset at: data/rule-reasoning-dataset-V2020.2.4/depth-5/dev.jsonl
2021-12-12 17:40:09,571 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNeg-D5-537', 'context': 'Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.', 'meta': {'sentenceScramble': [11, 8, 21, 10, 13, 15, 12, 3, 4, 19, 5, 17, 14, 1, 20, 6, 2, 18, 9, 7, 16]}, 'questions': [{'id': 'AttNeg-D5-537-1', 'text': 'Bob is cold.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNeg-D5-537-2', 'text': 'Harry is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNeg-D5-537-3', 'text': 'Harry is quiet.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNeg-D5-537-4', 'text': 'Harry is not quiet.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNeg-D5-537-5', 'text': 'Harry is blue.', 'label': True, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNeg-D5-537-6', 'text': 'Dave is not blue.', 'label': False, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNeg-D5-537-7', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNeg-D5-537-8', 'text': 'Harry is not round.', 'label': False, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNeg-D5-537-9', 'text': 'Harry is smart.', 'label': True, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNeg-D5-537-10', 'text': 'Harry is not smart.', 'label': False, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNeg-D5-537-11', 'text': 'Harry is not red.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNeg-D5-537-12', 'text': 'Harry is red.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNeg-D5-537-13', 'text': 'Dave is not cold.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNeg-D5-537-14', 'text': 'Dave is smart.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNeg-D5-537-15', 'text': 'Bob is not blue.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNeg-D5-537-16', 'text': 'Bob is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:40:09,573 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Bob, ƒ†is, ƒ†cold, ., </s>, </s>, ƒ†C, :, ƒ†Harry, ƒ†is, ƒ†cold, ., ƒ†Bob, ƒ†is, ƒ†red, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†not, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†If, ƒ†something, ƒ†is, ƒ†quiet, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†cold, ., ƒ†Harry, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†quiet, ., ƒ†Anne, ƒ†is, ƒ†round, ., ƒ†If, ƒ†something, ƒ†is, ƒ†cold, ƒ†and, ƒ†round, ƒ†then, ƒ†it, ƒ†is, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†quiet, ., ƒ†All, ƒ†blue, ƒ†things, ƒ†are, ƒ†round, ., ƒ†Anne, ƒ†is, ƒ†blue, ., ƒ†Smart, ,, ƒ†blue, ƒ†things, ƒ†are, ƒ†not, ƒ†red, ., ƒ†Bob, ƒ†is, ƒ†cold, ., ƒ†Anne, ƒ†is, ƒ†cold, ., ƒ†If, ƒ†Anne, ƒ†is, ƒ†cold, ƒ†and, ƒ†Anne, ƒ†is, ƒ†smart, ƒ†then, ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Bob, ƒ†is, ƒ†round, ., ƒ†Bob, ƒ†is, ƒ†not, ƒ†quiet, ., ƒ†All, ƒ†blue, ,, ƒ†quiet, ƒ†things, ƒ†are, ƒ†round, ., </s>]
2021-12-12 17:40:09,573 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.
2021-12-12 17:40:09,573 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 1
2021-12-12 17:40:09,573 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNeg-D5-537', 'context': 'Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.', 'meta': {'sentenceScramble': [11, 8, 21, 10, 13, 15, 12, 3, 4, 19, 5, 17, 14, 1, 20, 6, 2, 18, 9, 7, 16]}, 'questions': [{'id': 'AttNeg-D5-537-1', 'text': 'Bob is cold.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNeg-D5-537-2', 'text': 'Harry is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNeg-D5-537-3', 'text': 'Harry is quiet.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNeg-D5-537-4', 'text': 'Harry is not quiet.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNeg-D5-537-5', 'text': 'Harry is blue.', 'label': True, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNeg-D5-537-6', 'text': 'Dave is not blue.', 'label': False, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNeg-D5-537-7', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNeg-D5-537-8', 'text': 'Harry is not round.', 'label': False, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNeg-D5-537-9', 'text': 'Harry is smart.', 'label': True, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNeg-D5-537-10', 'text': 'Harry is not smart.', 'label': False, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNeg-D5-537-11', 'text': 'Harry is not red.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNeg-D5-537-12', 'text': 'Harry is red.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNeg-D5-537-13', 'text': 'Dave is not cold.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNeg-D5-537-14', 'text': 'Dave is smart.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNeg-D5-537-15', 'text': 'Bob is not blue.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNeg-D5-537-16', 'text': 'Bob is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:40:09,574 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Harry, ƒ†is, ƒ†not, ƒ†young, ., </s>, </s>, ƒ†C, :, ƒ†Harry, ƒ†is, ƒ†cold, ., ƒ†Bob, ƒ†is, ƒ†red, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†not, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†If, ƒ†something, ƒ†is, ƒ†quiet, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†cold, ., ƒ†Harry, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†quiet, ., ƒ†Anne, ƒ†is, ƒ†round, ., ƒ†If, ƒ†something, ƒ†is, ƒ†cold, ƒ†and, ƒ†round, ƒ†then, ƒ†it, ƒ†is, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†quiet, ., ƒ†All, ƒ†blue, ƒ†things, ƒ†are, ƒ†round, ., ƒ†Anne, ƒ†is, ƒ†blue, ., ƒ†Smart, ,, ƒ†blue, ƒ†things, ƒ†are, ƒ†not, ƒ†red, ., ƒ†Bob, ƒ†is, ƒ†cold, ., ƒ†Anne, ƒ†is, ƒ†cold, ., ƒ†If, ƒ†Anne, ƒ†is, ƒ†cold, ƒ†and, ƒ†Anne, ƒ†is, ƒ†smart, ƒ†then, ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Bob, ƒ†is, ƒ†round, ., ƒ†Bob, ƒ†is, ƒ†not, ƒ†quiet, ., ƒ†All, ƒ†blue, ,, ƒ†quiet, ƒ†things, ƒ†are, ƒ†round, ., </s>]
2021-12-12 17:40:09,575 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.
2021-12-12 17:40:09,575 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 0
2021-12-12 17:40:09,575 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNeg-D5-537', 'context': 'Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.', 'meta': {'sentenceScramble': [11, 8, 21, 10, 13, 15, 12, 3, 4, 19, 5, 17, 14, 1, 20, 6, 2, 18, 9, 7, 16]}, 'questions': [{'id': 'AttNeg-D5-537-1', 'text': 'Bob is cold.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNeg-D5-537-2', 'text': 'Harry is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNeg-D5-537-3', 'text': 'Harry is quiet.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNeg-D5-537-4', 'text': 'Harry is not quiet.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNeg-D5-537-5', 'text': 'Harry is blue.', 'label': True, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNeg-D5-537-6', 'text': 'Dave is not blue.', 'label': False, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNeg-D5-537-7', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNeg-D5-537-8', 'text': 'Harry is not round.', 'label': False, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNeg-D5-537-9', 'text': 'Harry is smart.', 'label': True, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNeg-D5-537-10', 'text': 'Harry is not smart.', 'label': False, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNeg-D5-537-11', 'text': 'Harry is not red.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNeg-D5-537-12', 'text': 'Harry is red.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNeg-D5-537-13', 'text': 'Dave is not cold.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNeg-D5-537-14', 'text': 'Dave is smart.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNeg-D5-537-15', 'text': 'Bob is not blue.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNeg-D5-537-16', 'text': 'Bob is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:40:09,576 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Harry, ƒ†is, ƒ†quiet, ., </s>, </s>, ƒ†C, :, ƒ†Harry, ƒ†is, ƒ†cold, ., ƒ†Bob, ƒ†is, ƒ†red, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†not, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†If, ƒ†something, ƒ†is, ƒ†quiet, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†cold, ., ƒ†Harry, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†quiet, ., ƒ†Anne, ƒ†is, ƒ†round, ., ƒ†If, ƒ†something, ƒ†is, ƒ†cold, ƒ†and, ƒ†round, ƒ†then, ƒ†it, ƒ†is, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†quiet, ., ƒ†All, ƒ†blue, ƒ†things, ƒ†are, ƒ†round, ., ƒ†Anne, ƒ†is, ƒ†blue, ., ƒ†Smart, ,, ƒ†blue, ƒ†things, ƒ†are, ƒ†not, ƒ†red, ., ƒ†Bob, ƒ†is, ƒ†cold, ., ƒ†Anne, ƒ†is, ƒ†cold, ., ƒ†If, ƒ†Anne, ƒ†is, ƒ†cold, ƒ†and, ƒ†Anne, ƒ†is, ƒ†smart, ƒ†then, ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Bob, ƒ†is, ƒ†round, ., ƒ†Bob, ƒ†is, ƒ†not, ƒ†quiet, ., ƒ†All, ƒ†blue, ,, ƒ†quiet, ƒ†things, ƒ†are, ƒ†round, ., </s>]
2021-12-12 17:40:09,576 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.
2021-12-12 17:40:09,577 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 1
2021-12-12 17:40:09,577 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'AttNeg-D5-537', 'context': 'Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.', 'meta': {'sentenceScramble': [11, 8, 21, 10, 13, 15, 12, 3, 4, 19, 5, 17, 14, 1, 20, 6, 2, 18, 9, 7, 16]}, 'questions': [{'id': 'AttNeg-D5-537-1', 'text': 'Bob is cold.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'AttNeg-D5-537-2', 'text': 'Harry is not young.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'AttNeg-D5-537-3', 'text': 'Harry is quiet.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'AttNeg-D5-537-4', 'text': 'Harry is not quiet.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'AttNeg-D5-537-5', 'text': 'Harry is blue.', 'label': True, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'AttNeg-D5-537-6', 'text': 'Dave is not blue.', 'label': False, 'meta': {'QDep': 2, 'QLen': 3, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'AttNeg-D5-537-7', 'text': 'Dave is round.', 'label': True, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'AttNeg-D5-537-8', 'text': 'Harry is not round.', 'label': False, 'meta': {'QDep': 3, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'AttNeg-D5-537-9', 'text': 'Harry is smart.', 'label': True, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'AttNeg-D5-537-10', 'text': 'Harry is not smart.', 'label': False, 'meta': {'QDep': 4, 'QLen': 6, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'AttNeg-D5-537-11', 'text': 'Harry is not red.', 'label': True, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'AttNeg-D5-537-12', 'text': 'Harry is red.', 'label': False, 'meta': {'QDep': 5, 'QLen': 10, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'AttNeg-D5-537-13', 'text': 'Dave is not cold.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'AttNeg-D5-537-14', 'text': 'Dave is smart.', 'label': False, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'AttNeg-D5-537-15', 'text': 'Bob is not blue.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'AttNeg-D5-537-16', 'text': 'Bob is young.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q16'}}]}
2021-12-12 17:40:09,578 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†Harry, ƒ†is, ƒ†not, ƒ†quiet, ., </s>, </s>, ƒ†C, :, ƒ†Harry, ƒ†is, ƒ†cold, ., ƒ†Bob, ƒ†is, ƒ†red, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†not, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†Dave, ƒ†is, ƒ†young, ., ƒ†If, ƒ†something, ƒ†is, ƒ†quiet, ƒ†then, ƒ†it, ƒ†is, ƒ†blue, ., ƒ†If, ƒ†something, ƒ†is, ƒ†round, ƒ†and, ƒ†smart, ƒ†then, ƒ†it, ƒ†is, ƒ†cold, ., ƒ†Harry, ƒ†is, ƒ†young, ., ƒ†Anne, ƒ†is, ƒ†quiet, ., ƒ†Anne, ƒ†is, ƒ†round, ., ƒ†If, ƒ†something, ƒ†is, ƒ†cold, ƒ†and, ƒ†round, ƒ†then, ƒ†it, ƒ†is, ƒ†smart, ., ƒ†Anne, ƒ†is, ƒ†smart, ., ƒ†All, ƒ†young, ƒ†things, ƒ†are, ƒ†quiet, ., ƒ†All, ƒ†blue, ƒ†things, ƒ†are, ƒ†round, ., ƒ†Anne, ƒ†is, ƒ†blue, ., ƒ†Smart, ,, ƒ†blue, ƒ†things, ƒ†are, ƒ†not, ƒ†red, ., ƒ†Bob, ƒ†is, ƒ†cold, ., ƒ†Anne, ƒ†is, ƒ†cold, ., ƒ†If, ƒ†Anne, ƒ†is, ƒ†cold, ƒ†and, ƒ†Anne, ƒ†is, ƒ†smart, ƒ†then, ƒ†Anne, ƒ†is, ƒ†young, ., ƒ†Bob, ƒ†is, ƒ†round, ., ƒ†Bob, ƒ†is, ƒ†not, ƒ†quiet, ., ƒ†All, ƒ†blue, ,, ƒ†quiet, ƒ†things, ƒ†are, ƒ†round, ., </s>]
2021-12-12 17:40:09,578 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = Harry is cold. Bob is red. If something is round and not smart then it is blue. Dave is young. If something is quiet then it is blue. If something is round and smart then it is cold. Harry is young. Anne is quiet. Anne is round. If something is cold and round then it is smart. Anne is smart. All young things are quiet. All blue things are round. Anne is blue. Smart, blue things are not red. Bob is cold. Anne is cold. If Anne is cold and Anne is smart then Anne is young. Bob is round. Bob is not quiet. All blue, quiet things are round.
2021-12-12 17:40:09,578 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 0
97it [00:00, 965.56it/s]200it [00:00, 1001.78it/s]301it [00:00, 688.00it/s] 388it [00:00, 743.42it/s]503it [00:00, 866.23it/s]624it [00:00, 968.12it/s]727it [00:00, 802.29it/s]829it [00:00, 857.49it/s]969it [00:01, 1002.38it/s]1090it [00:01, 1060.12it/s]1202it [00:01, 1046.25it/s]1311it [00:01, 1023.19it/s]1430it [00:01, 1069.19it/s]1559it [00:01, 1130.04it/s]1674it [00:01, 1128.89it/s]1789it [00:01, 1105.55it/s]1901it [00:01, 1096.09it/s]2017it [00:02, 1112.36it/s]2129it [00:02, 1060.76it/s]2251it [00:02, 1105.67it/s]2364it [00:02, 1110.36it/s]2489it [00:02, 1149.15it/s]2605it [00:02, 1108.16it/s]2733it [00:02, 1154.63it/s]2850it [00:02, 1147.09it/s]2966it [00:02, 1075.92it/s]3075it [00:02, 1060.78it/s]3189it [00:03, 1082.76it/s]3298it [00:03, 1078.56it/s]3407it [00:03, 1031.35it/s]3511it [00:03, 1004.81it/s]3647it [00:03, 1103.47it/s]3759it [00:03, 1076.65it/s]3868it [00:03, 1057.30it/s]3977it [00:03, 1064.98it/s]4095it [00:03, 1098.04it/s]4224it [00:04, 1150.75it/s]4340it [00:04, 1111.69it/s]4460it [00:04, 1136.21it/s]4575it [00:04, 1137.29it/s]4692it [00:04, 1146.00it/s]4807it [00:04, 1099.87it/s]4931it [00:04, 1136.36it/s]5046it [00:04, 1075.19it/s]5164it [00:04, 1103.88it/s]5283it [00:04, 1127.20it/s]5397it [00:05, 1087.79it/s]5537it [00:05, 1174.41it/s]5656it [00:05, 1113.44it/s]5769it [00:05, 1085.02it/s]5884it [00:05, 1102.76it/s]5995it [00:05, 1058.59it/s]6105it [00:05, 1069.79it/s]6215it [00:05, 1077.35it/s]6360it [00:05, 1183.64it/s]6479it [00:06, 1134.06it/s]6597it [00:06, 1146.26it/s]6716it [00:06, 1156.14it/s]6833it [00:06, 1098.56it/s]6944it [00:06, 1096.06it/s]7055it [00:06, 1094.62it/s]7171it [00:06, 1111.77it/s]7292it [00:06, 1136.48it/s]7406it [00:06, 1079.72it/s]7527it [00:07, 1115.63it/s]7640it [00:07, 1107.91it/s]7752it [00:07, 1098.62it/s]7873it [00:07, 1127.90it/s]7987it [00:07, 1128.02it/s]8104it [00:07, 1139.18it/s]8220it [00:07, 1143.75it/s]8335it [00:07, 1144.11it/s]8456it [00:07, 1160.74it/s]8575it [00:07, 1168.43it/s]8692it [00:08, 1164.57it/s]8811it [00:08, 1170.87it/s]8929it [00:08, 1170.91it/s]9047it [00:08, 1097.36it/s]9170it [00:08, 1134.80it/s]9292it [00:08, 1158.16it/s]9409it [00:08, 1142.86it/s]9524it [00:08, 1124.97it/s]9646it [00:08, 1151.20it/s]9762it [00:08, 1111.85it/s]9884it [00:09, 1142.64it/s]9999it [00:09, 1125.23it/s]10068it [00:09, 1089.16it/s]
2021-12-12 17:40:18,814 - INFO - allennlp.training.util - Reading test data from data/rule-reasoning-dataset-V2020.2.4/depth-5/test.jsonl
0it [00:00, ?it/s]2021-12-12 17:40:18,816 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - Reading instances from jsonl dataset at: data/rule-reasoning-dataset-V2020.2.4/depth-5/test.jsonl
2021-12-12 17:40:18,816 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'RelNeg-D5-736', 'context': 'The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.', 'meta': {'sentenceScramble': [3, 14, 10, 6, 15, 20, 13, 5, 17, 7, 1, 19, 9, 12, 18, 4, 16, 8, 2, 11]}, 'questions': [{'id': 'RelNeg-D5-736-1', 'text': 'The mouse does not visit the lion.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'RelNeg-D5-736-2', 'text': 'The lion does not visit the rabbit.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'RelNeg-D5-736-3', 'text': 'The cat is big.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'RelNeg-D5-736-4', 'text': 'The rabbit is kind.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'RelNeg-D5-736-5', 'text': 'The mouse chases the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'RelNeg-D5-736-6', 'text': 'The mouse does not chase the cat.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'RelNeg-D5-736-7', 'text': 'The cat sees the lion.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'RelNeg-D5-736-8', 'text': 'The cat does not see the lion.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'RelNeg-D5-736-9', 'text': 'The cat is kind.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'RelNeg-D5-736-10', 'text': 'The cat is not kind.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'RelNeg-D5-736-11', 'text': 'The cat does not visit the cat.', 'label': True, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'RelNeg-D5-736-12', 'text': 'The cat visits the cat.', 'label': False, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'RelNeg-D5-736-13', 'text': 'The lion is not big.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'RelNeg-D5-736-14', 'text': 'The rabbit chases the cat.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'RelNeg-D5-736-15', 'text': 'The lion does not chase the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'RelNeg-D5-736-16', 'text': 'The mouse sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q16'}}, {'id': 'RelNeg-D5-736-17', 'text': 'The lion is not kind.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q17'}}, {'id': 'RelNeg-D5-736-18', 'text': 'The rabbit sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q18'}}, {'id': 'RelNeg-D5-736-19', 'text': 'The mouse does not visit the mouse.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q19'}}, {'id': 'RelNeg-D5-736-20', 'text': 'The rabbit is red.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q20'}}, {'id': 'RelNeg-D5-736-21', 'text': 'The mouse is not blue.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q21'}}, {'id': 'RelNeg-D5-736-22', 'text': 'The lion sees the cat.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q22'}}, {'id': 'RelNeg-D5-736-23', 'text': 'The mouse is not red.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q23'}}, {'id': 'RelNeg-D5-736-24', 'text': 'The rabbit chases the lion.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q24'}}]}
2021-12-12 17:40:18,818 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†lion, ., </s>, </s>, ƒ†C, :, ƒ†The, ƒ†cat, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†rabbit, ƒ†then, ƒ†it, ƒ†is, ƒ†big, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†lion, ƒ†visits, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†cat, ƒ†then, ƒ†the, ƒ†cat, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†If, ƒ†something, ƒ†is, ƒ†kind, ƒ†then, ƒ†it, ƒ†ch, ases, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†cat, ƒ†then, ƒ†it, ƒ†is, ƒ†not, ƒ†kind, ., ƒ†The, ƒ†lion, ƒ†is, ƒ†green, ., ƒ†If, ƒ†something, ƒ†is, ƒ†green, ƒ†then, ƒ†it, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†see, ƒ†the, ƒ†cat, ., ƒ†The, ƒ†cat, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†lion, ƒ†and, ƒ†it, ƒ†is, ƒ†not, ƒ†blue, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†The, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†is, ƒ†red, ƒ†and, ƒ†kind, ƒ†then, ƒ†it, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ƒ†and, ƒ†it, ƒ†sees, ƒ†the, ƒ†mouse, ƒ†then, ƒ†the, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†visits, ƒ†the, ƒ†mouse, ., ƒ†If, ƒ†the, ƒ†rabbit, ƒ†visits, ƒ†the, ƒ†mouse, ƒ†and, ƒ†the, ƒ†rabbit, ƒ†is, ƒ†big, ƒ†then, ƒ†the, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†is, ƒ†red, ., ƒ†The, ƒ†rabbit, ƒ†sees, ƒ†the, ƒ†cat, ., </s>]
2021-12-12 17:40:18,818 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.
2021-12-12 17:40:18,818 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 1
2021-12-12 17:40:18,818 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'RelNeg-D5-736', 'context': 'The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.', 'meta': {'sentenceScramble': [3, 14, 10, 6, 15, 20, 13, 5, 17, 7, 1, 19, 9, 12, 18, 4, 16, 8, 2, 11]}, 'questions': [{'id': 'RelNeg-D5-736-1', 'text': 'The mouse does not visit the lion.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'RelNeg-D5-736-2', 'text': 'The lion does not visit the rabbit.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'RelNeg-D5-736-3', 'text': 'The cat is big.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'RelNeg-D5-736-4', 'text': 'The rabbit is kind.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'RelNeg-D5-736-5', 'text': 'The mouse chases the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'RelNeg-D5-736-6', 'text': 'The mouse does not chase the cat.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'RelNeg-D5-736-7', 'text': 'The cat sees the lion.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'RelNeg-D5-736-8', 'text': 'The cat does not see the lion.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'RelNeg-D5-736-9', 'text': 'The cat is kind.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'RelNeg-D5-736-10', 'text': 'The cat is not kind.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'RelNeg-D5-736-11', 'text': 'The cat does not visit the cat.', 'label': True, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'RelNeg-D5-736-12', 'text': 'The cat visits the cat.', 'label': False, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'RelNeg-D5-736-13', 'text': 'The lion is not big.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'RelNeg-D5-736-14', 'text': 'The rabbit chases the cat.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'RelNeg-D5-736-15', 'text': 'The lion does not chase the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'RelNeg-D5-736-16', 'text': 'The mouse sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q16'}}, {'id': 'RelNeg-D5-736-17', 'text': 'The lion is not kind.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q17'}}, {'id': 'RelNeg-D5-736-18', 'text': 'The rabbit sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q18'}}, {'id': 'RelNeg-D5-736-19', 'text': 'The mouse does not visit the mouse.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q19'}}, {'id': 'RelNeg-D5-736-20', 'text': 'The rabbit is red.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q20'}}, {'id': 'RelNeg-D5-736-21', 'text': 'The mouse is not blue.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q21'}}, {'id': 'RelNeg-D5-736-22', 'text': 'The lion sees the cat.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q22'}}, {'id': 'RelNeg-D5-736-23', 'text': 'The mouse is not red.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q23'}}, {'id': 'RelNeg-D5-736-24', 'text': 'The rabbit chases the lion.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q24'}}]}
2021-12-12 17:40:18,820 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†The, ƒ†lion, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†rabbit, ., </s>, </s>, ƒ†C, :, ƒ†The, ƒ†cat, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†rabbit, ƒ†then, ƒ†it, ƒ†is, ƒ†big, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†lion, ƒ†visits, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†cat, ƒ†then, ƒ†the, ƒ†cat, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†If, ƒ†something, ƒ†is, ƒ†kind, ƒ†then, ƒ†it, ƒ†ch, ases, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†cat, ƒ†then, ƒ†it, ƒ†is, ƒ†not, ƒ†kind, ., ƒ†The, ƒ†lion, ƒ†is, ƒ†green, ., ƒ†If, ƒ†something, ƒ†is, ƒ†green, ƒ†then, ƒ†it, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†see, ƒ†the, ƒ†cat, ., ƒ†The, ƒ†cat, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†lion, ƒ†and, ƒ†it, ƒ†is, ƒ†not, ƒ†blue, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†The, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†is, ƒ†red, ƒ†and, ƒ†kind, ƒ†then, ƒ†it, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ƒ†and, ƒ†it, ƒ†sees, ƒ†the, ƒ†mouse, ƒ†then, ƒ†the, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†visits, ƒ†the, ƒ†mouse, ., ƒ†If, ƒ†the, ƒ†rabbit, ƒ†visits, ƒ†the, ƒ†mouse, ƒ†and, ƒ†the, ƒ†rabbit, ƒ†is, ƒ†big, ƒ†then, ƒ†the, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†is, ƒ†red, ., ƒ†The, ƒ†rabbit, ƒ†sees, ƒ†the, ƒ†cat, ., </s>]
2021-12-12 17:40:18,820 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.
2021-12-12 17:40:18,820 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 0
2021-12-12 17:40:18,820 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'RelNeg-D5-736', 'context': 'The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.', 'meta': {'sentenceScramble': [3, 14, 10, 6, 15, 20, 13, 5, 17, 7, 1, 19, 9, 12, 18, 4, 16, 8, 2, 11]}, 'questions': [{'id': 'RelNeg-D5-736-1', 'text': 'The mouse does not visit the lion.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'RelNeg-D5-736-2', 'text': 'The lion does not visit the rabbit.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'RelNeg-D5-736-3', 'text': 'The cat is big.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'RelNeg-D5-736-4', 'text': 'The rabbit is kind.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'RelNeg-D5-736-5', 'text': 'The mouse chases the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'RelNeg-D5-736-6', 'text': 'The mouse does not chase the cat.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'RelNeg-D5-736-7', 'text': 'The cat sees the lion.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'RelNeg-D5-736-8', 'text': 'The cat does not see the lion.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'RelNeg-D5-736-9', 'text': 'The cat is kind.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'RelNeg-D5-736-10', 'text': 'The cat is not kind.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'RelNeg-D5-736-11', 'text': 'The cat does not visit the cat.', 'label': True, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'RelNeg-D5-736-12', 'text': 'The cat visits the cat.', 'label': False, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'RelNeg-D5-736-13', 'text': 'The lion is not big.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'RelNeg-D5-736-14', 'text': 'The rabbit chases the cat.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'RelNeg-D5-736-15', 'text': 'The lion does not chase the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'RelNeg-D5-736-16', 'text': 'The mouse sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q16'}}, {'id': 'RelNeg-D5-736-17', 'text': 'The lion is not kind.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q17'}}, {'id': 'RelNeg-D5-736-18', 'text': 'The rabbit sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q18'}}, {'id': 'RelNeg-D5-736-19', 'text': 'The mouse does not visit the mouse.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q19'}}, {'id': 'RelNeg-D5-736-20', 'text': 'The rabbit is red.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q20'}}, {'id': 'RelNeg-D5-736-21', 'text': 'The mouse is not blue.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q21'}}, {'id': 'RelNeg-D5-736-22', 'text': 'The lion sees the cat.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q22'}}, {'id': 'RelNeg-D5-736-23', 'text': 'The mouse is not red.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q23'}}, {'id': 'RelNeg-D5-736-24', 'text': 'The rabbit chases the lion.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q24'}}]}
2021-12-12 17:40:18,822 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†The, ƒ†cat, ƒ†is, ƒ†big, ., </s>, </s>, ƒ†C, :, ƒ†The, ƒ†cat, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†rabbit, ƒ†then, ƒ†it, ƒ†is, ƒ†big, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†lion, ƒ†visits, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†cat, ƒ†then, ƒ†the, ƒ†cat, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†If, ƒ†something, ƒ†is, ƒ†kind, ƒ†then, ƒ†it, ƒ†ch, ases, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†cat, ƒ†then, ƒ†it, ƒ†is, ƒ†not, ƒ†kind, ., ƒ†The, ƒ†lion, ƒ†is, ƒ†green, ., ƒ†If, ƒ†something, ƒ†is, ƒ†green, ƒ†then, ƒ†it, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†see, ƒ†the, ƒ†cat, ., ƒ†The, ƒ†cat, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†lion, ƒ†and, ƒ†it, ƒ†is, ƒ†not, ƒ†blue, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†The, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†is, ƒ†red, ƒ†and, ƒ†kind, ƒ†then, ƒ†it, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ƒ†and, ƒ†it, ƒ†sees, ƒ†the, ƒ†mouse, ƒ†then, ƒ†the, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†visits, ƒ†the, ƒ†mouse, ., ƒ†If, ƒ†the, ƒ†rabbit, ƒ†visits, ƒ†the, ƒ†mouse, ƒ†and, ƒ†the, ƒ†rabbit, ƒ†is, ƒ†big, ƒ†then, ƒ†the, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†is, ƒ†red, ., ƒ†The, ƒ†rabbit, ƒ†sees, ƒ†the, ƒ†cat, ., </s>]
2021-12-12 17:40:18,822 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.
2021-12-12 17:40:18,822 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 1
2021-12-12 17:40:18,822 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - {'id': 'RelNeg-D5-736', 'context': 'The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.', 'meta': {'sentenceScramble': [3, 14, 10, 6, 15, 20, 13, 5, 17, 7, 1, 19, 9, 12, 18, 4, 16, 8, 2, 11]}, 'questions': [{'id': 'RelNeg-D5-736-1', 'text': 'The mouse does not visit the lion.', 'label': True, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'proof', 'Qid': 'Q1'}}, {'id': 'RelNeg-D5-736-2', 'text': 'The lion does not visit the rabbit.', 'label': False, 'meta': {'QDep': 0, 'QLen': 1, 'strategy': 'inv-proof', 'Qid': 'Q2'}}, {'id': 'RelNeg-D5-736-3', 'text': 'The cat is big.', 'label': True, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'proof', 'Qid': 'Q3'}}, {'id': 'RelNeg-D5-736-4', 'text': 'The rabbit is kind.', 'label': False, 'meta': {'QDep': 1, 'QLen': 2, 'strategy': 'inv-proof', 'Qid': 'Q4'}}, {'id': 'RelNeg-D5-736-5', 'text': 'The mouse chases the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'proof', 'Qid': 'Q5'}}, {'id': 'RelNeg-D5-736-6', 'text': 'The mouse does not chase the cat.', 'label': False, 'meta': {'QDep': 2, 'QLen': 4, 'strategy': 'inv-proof', 'Qid': 'Q6'}}, {'id': 'RelNeg-D5-736-7', 'text': 'The cat sees the lion.', 'label': True, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'proof', 'Qid': 'Q7'}}, {'id': 'RelNeg-D5-736-8', 'text': 'The cat does not see the lion.', 'label': False, 'meta': {'QDep': 3, 'QLen': 5, 'strategy': 'inv-proof', 'Qid': 'Q8'}}, {'id': 'RelNeg-D5-736-9', 'text': 'The cat is kind.', 'label': True, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'proof', 'Qid': 'Q9'}}, {'id': 'RelNeg-D5-736-10', 'text': 'The cat is not kind.', 'label': False, 'meta': {'QDep': 4, 'QLen': 7, 'strategy': 'inv-proof', 'Qid': 'Q10'}}, {'id': 'RelNeg-D5-736-11', 'text': 'The cat does not visit the cat.', 'label': True, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'proof', 'Qid': 'Q11'}}, {'id': 'RelNeg-D5-736-12', 'text': 'The cat visits the cat.', 'label': False, 'meta': {'QDep': 5, 'QLen': 9, 'strategy': 'inv-proof', 'Qid': 'Q12'}}, {'id': 'RelNeg-D5-736-13', 'text': 'The lion is not big.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q13'}}, {'id': 'RelNeg-D5-736-14', 'text': 'The rabbit chases the cat.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q14'}}, {'id': 'RelNeg-D5-736-15', 'text': 'The lion does not chase the cat.', 'label': True, 'meta': {'QDep': 2, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q15'}}, {'id': 'RelNeg-D5-736-16', 'text': 'The mouse sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q16'}}, {'id': 'RelNeg-D5-736-17', 'text': 'The lion is not kind.', 'label': True, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'inv-rconc', 'Qid': 'Q17'}}, {'id': 'RelNeg-D5-736-18', 'text': 'The rabbit sees the rabbit.', 'label': False, 'meta': {'QDep': 1, 'QLen': '', 'strategy': 'rconc', 'Qid': 'Q18'}}, {'id': 'RelNeg-D5-736-19', 'text': 'The mouse does not visit the mouse.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q19'}}, {'id': 'RelNeg-D5-736-20', 'text': 'The rabbit is red.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q20'}}, {'id': 'RelNeg-D5-736-21', 'text': 'The mouse is not blue.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q21'}}, {'id': 'RelNeg-D5-736-22', 'text': 'The lion sees the cat.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q22'}}, {'id': 'RelNeg-D5-736-23', 'text': 'The mouse is not red.', 'label': True, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'inv-random', 'Qid': 'Q23'}}, {'id': 'RelNeg-D5-736-24', 'text': 'The rabbit chases the lion.', 'label': False, 'meta': {'QDep': 0, 'QLen': '', 'strategy': 'random', 'Qid': 'Q24'}}]}
2021-12-12 17:40:18,824 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - qa_tokens = [<s>, ƒ†Q, :, ƒ†The, ƒ†rabbit, ƒ†is, ƒ†kind, ., </s>, </s>, ƒ†C, :, ƒ†The, ƒ†cat, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†rabbit, ƒ†then, ƒ†it, ƒ†is, ƒ†big, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†lion, ƒ†visits, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†cat, ƒ†then, ƒ†the, ƒ†cat, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†If, ƒ†something, ƒ†is, ƒ†kind, ƒ†then, ƒ†it, ƒ†ch, ases, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†cat, ƒ†then, ƒ†it, ƒ†is, ƒ†not, ƒ†kind, ., ƒ†The, ƒ†lion, ƒ†is, ƒ†green, ., ƒ†If, ƒ†something, ƒ†is, ƒ†green, ƒ†then, ƒ†it, ƒ†sees, ƒ†the, ƒ†rabbit, ., ƒ†The, ƒ†mouse, ƒ†does, ƒ†not, ƒ†see, ƒ†the, ƒ†cat, ., ƒ†The, ƒ†cat, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ., ƒ†If, ƒ†something, ƒ†sees, ƒ†the, ƒ†lion, ƒ†and, ƒ†it, ƒ†is, ƒ†not, ƒ†blue, ƒ†then, ƒ†it, ƒ†is, ƒ†kind, ., ƒ†The, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†is, ƒ†red, ƒ†and, ƒ†kind, ƒ†then, ƒ†it, ƒ†does, ƒ†not, ƒ†visit, ƒ†the, ƒ†cat, ., ƒ†If, ƒ†something, ƒ†ch, ases, ƒ†the, ƒ†rabbit, ƒ†and, ƒ†it, ƒ†sees, ƒ†the, ƒ†mouse, ƒ†then, ƒ†the, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†visits, ƒ†the, ƒ†mouse, ., ƒ†If, ƒ†the, ƒ†rabbit, ƒ†visits, ƒ†the, ƒ†mouse, ƒ†and, ƒ†the, ƒ†rabbit, ƒ†is, ƒ†big, ƒ†then, ƒ†the, ƒ†mouse, ƒ†visits, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†mouse, ƒ†sees, ƒ†the, ƒ†lion, ., ƒ†The, ƒ†cat, ƒ†is, ƒ†red, ., ƒ†The, ƒ†rabbit, ƒ†sees, ƒ†the, ƒ†cat, ., </s>]
2021-12-12 17:40:18,825 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - context = The cat sees the rabbit. If something does not visit the rabbit then it is big. The mouse does not visit the lion. The lion visits the rabbit. If something chases the cat then the cat sees the lion. If something is kind then it chases the cat. If something sees the cat then it is not kind. The lion is green. If something is green then it sees the rabbit. The mouse does not see the cat. The cat chases the rabbit. If something sees the lion and it is not blue then it is kind. The mouse visits the cat. If something is red and kind then it does not visit the cat. If something chases the rabbit and it sees the mouse then the mouse sees the lion. The cat visits the mouse. If the rabbit visits the mouse and the rabbit is big then the mouse visits the lion. The mouse sees the lion. The cat is red. The rabbit sees the cat.
2021-12-12 17:40:18,825 - INFO - ruletaker.allennlp_models.dataset_readers.rule_reasoning_reader - label = 0
96it [00:00, 959.38it/s]192it [00:00, 539.88it/s]311it [00:00, 747.80it/s]407it [00:00, 814.31it/s]513it [00:00, 887.98it/s]624it [00:00, 955.48it/s]740it [00:00, 1016.92it/s]858it [00:00, 1065.18it/s]969it [00:01, 1077.63it/s]1087it [00:01, 1107.15it/s]1210it [00:01, 1143.87it/s]1326it [00:01, 1083.58it/s]1436it [00:01, 1082.50it/s]1546it [00:01, 1016.43it/s]1650it [00:01, 814.27it/s] 1754it [00:01, 867.08it/s]1861it [00:01, 917.99it/s]1971it [00:02, 966.02it/s]2079it [00:02, 996.35it/s]2190it [00:02, 1026.40it/s]2320it [00:02, 1103.49it/s]2433it [00:02, 1085.03it/s]2549it [00:02, 1105.39it/s]2661it [00:02, 1055.12it/s]2768it [00:02, 1055.43it/s]2886it [00:02, 1089.64it/s]2996it [00:03, 1030.08it/s]3101it [00:06, 112.54it/s] 3201it [00:06, 149.66it/s]3308it [00:06, 201.58it/s]3430it [00:06, 277.36it/s]3529it [00:06, 342.72it/s]3626it [00:06, 414.42it/s]3722it [00:06, 493.28it/s]3821it [00:06, 577.49it/s]3917it [00:06, 651.55it/s]4013it [00:06, 716.80it/s]4124it [00:07, 810.12it/s]4252it [00:07, 927.73it/s]4361it [00:07, 959.94it/s]4469it [00:07, 946.48it/s]4572it [00:07, 943.74it/s]4687it [00:07, 998.66it/s]4810it [00:07, 1061.95it/s]4922it [00:07, 1077.40it/s]5033it [00:07, 1043.58it/s]5140it [00:08, 968.46it/s] 5272it [00:08, 1063.33it/s]5381it [00:08, 1049.83it/s]5489it [00:08, 1057.61it/s]5597it [00:08, 1048.41it/s]5703it [00:08, 1036.04it/s]5820it [00:08, 1072.77it/s]5928it [00:08, 1069.41it/s]6043it [00:08, 1091.17it/s]6153it [00:08, 1068.58it/s]6264it [00:09, 1079.76it/s]6387it [00:09, 1123.49it/s]6504it [00:09, 1134.11it/s]6618it [00:09, 1131.65it/s]6732it [00:09, 1110.70it/s]6844it [00:09, 1014.54it/s]6948it [00:09, 952.38it/s] 7045it [00:09, 927.13it/s]7151it [00:09, 960.66it/s]7266it [00:10, 1010.90it/s]7389it [00:10, 1072.83it/s]7504it [00:10, 1092.62it/s]7615it [00:10, 1092.36it/s]7732it [00:10, 1114.25it/s]7844it [00:10, 1086.91it/s]7963it [00:10, 1113.41it/s]8075it [00:10, 1091.27it/s]8192it [00:10, 1112.42it/s]8304it [00:10, 1097.33it/s]8414it [00:11, 1067.91it/s]8543it [00:11, 1128.91it/s]8657it [00:11, 1083.20it/s]8769it [00:11, 1092.53it/s]8889it [00:11, 1122.13it/s]9002it [00:11, 1109.82it/s]9114it [00:11, 1084.92it/s]9223it [00:11, 1079.03it/s]9332it [00:11, 1071.65it/s]9440it [00:12, 1004.63it/s]9549it [00:12, 1027.19it/s]9653it [00:12, 1022.69it/s]9756it [00:12, 994.45it/s] 9876it [00:12, 1050.96it/s]9982it [00:12, 1046.72it/s]10088it [00:12, 1021.15it/s]10191it [00:12, 996.63it/s] 10311it [00:12, 1052.43it/s]10417it [00:12, 1052.38it/s]10523it [00:13, 1041.58it/s]10628it [00:13, 1031.71it/s]10744it [00:13, 1067.99it/s]10874it [00:13, 1135.03it/s]10998it [00:13, 1164.28it/s]11115it [00:13, 1138.92it/s]11230it [00:13, 1116.46it/s]11342it [00:13, 1109.51it/s]11454it [00:13, 1078.76it/s]11564it [00:14, 1081.77it/s]11673it [00:14, 1074.79it/s]11820it [00:14, 1189.71it/s]11940it [00:14, 1180.42it/s]12063it [00:14, 1193.24it/s]12183it [00:14, 1125.97it/s]12304it [00:14, 1149.67it/s]12420it [00:14, 1082.29it/s]12539it [00:14, 1110.49it/s]12652it [00:14, 1093.18it/s]12763it [00:15, 1084.52it/s]12885it [00:15, 1122.35it/s]13005it [00:15, 1141.94it/s]13120it [00:15, 1077.12it/s]13238it [00:15, 1106.11it/s]13355it [00:15, 1122.19it/s]13468it [00:15, 1080.00it/s]13584it [00:15, 1100.67it/s]13695it [00:15, 1085.73it/s]13807it [00:16, 1093.63it/s]13917it [00:16, 1045.39it/s]14023it [00:16, 1042.86it/s]14128it [00:16, 1030.14it/s]14264it [00:16, 1123.83it/s]14377it [00:16, 1076.42it/s]14486it [00:16, 1067.82it/s]14594it [00:16, 1066.80it/s]14703it [00:16, 1072.60it/s]14827it [00:16, 1121.20it/s]14944it [00:17, 1134.79it/s]15058it [00:17, 1118.45it/s]15172it [00:17, 1122.19it/s]15286it [00:17, 1127.10it/s]15405it [00:17, 1143.79it/s]15520it [00:17, 1107.72it/s]15632it [00:17, 1056.88it/s]15769it [00:17, 1142.70it/s]15888it [00:17, 1155.74it/s]16005it [00:18, 1133.99it/s]16119it [00:18, 1134.23it/s]16237it [00:18, 1145.96it/s]16352it [00:18, 1133.55it/s]16473it [00:18, 1152.57it/s]16589it [00:18, 1140.81it/s]16704it [00:18, 1131.21it/s]16818it [00:18, 1125.28it/s]16931it [00:18, 1086.67it/s]17049it [00:18, 1112.97it/s]17161it [00:19, 1055.80it/s]17274it [00:19, 1074.60it/s]17383it [00:19, 1053.45it/s]17492it [00:19, 1063.11it/s]17599it [00:19, 1055.73it/s]17705it [00:19, 1042.32it/s]17812it [00:19, 1048.34it/s]17935it [00:19, 1100.76it/s]18046it [00:19, 1062.25it/s]18153it [00:20, 1059.23it/s]18271it [00:20, 1093.38it/s]18381it [00:20, 1071.62it/s]18501it [00:20, 1108.86it/s]18613it [00:20, 1105.17it/s]18724it [00:20, 1067.59it/s]18832it [00:20, 1067.73it/s]18945it [00:20, 1085.65it/s]19065it [00:20, 1118.98it/s]19178it [00:20, 1107.93it/s]19289it [00:21, 1088.31it/s]19399it [00:21, 1071.52it/s]19507it [00:21, 1071.70it/s]19615it [00:21, 1043.06it/s]19720it [00:21, 1044.71it/s]19844it [00:21, 1099.68it/s]19955it [00:21, 1036.57it/s]20072it [00:21, 1074.21it/s]20189it [00:21, 1100.18it/s]20192it [00:21, 922.73it/s] 2021-12-12 17:40:40,698 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.

0it [00:00, ?it/s]4279it [00:00, 42784.84it/s]8597it [00:00, 43014.04it/s]12942it [00:00, 43212.31it/s]17332it [00:00, 43481.93it/s]22059it [00:00, 44843.20it/s]26544it [00:00, 44457.79it/s]30991it [00:00, 41264.46it/s]35158it [00:00, 39074.33it/s]39107it [00:00, 38342.66it/s]42968it [00:01, 37796.74it/s]46765it [00:01, 37578.63it/s]50635it [00:01, 37900.83it/s]55189it [00:01, 40136.70it/s]59758it [00:01, 41773.12it/s]64021it [00:01, 42024.24it/s]68566it [00:01, 43041.63it/s]73176it [00:01, 43950.98it/s]77949it [00:01, 45078.25it/s]82659it [00:01, 45678.50it/s]87231it [00:02, 45238.71it/s]91759it [00:02, 44937.05it/s]96256it [00:02, 44872.01it/s]100022it [00:02, 42560.35it/s]2021-12-12 17:40:43,049 - INFO - allennlp.common.params - model.type = transformer_binary_qa
2021-12-12 17:40:43,049 - INFO - allennlp.common.params - model.pretrained_model = roberta-large
2021-12-12 17:40:43,049 - INFO - allennlp.common.params - model.requires_grad = True
2021-12-12 17:40:43,050 - INFO - allennlp.common.params - model.transformer_weights_model = 
2021-12-12 17:40:43,050 - INFO - allennlp.common.params - model.num_labels = 2
2021-12-12 17:40:43,050 - INFO - allennlp.common.params - model.predictions_file = None
2021-12-12 17:40:43,050 - INFO - allennlp.common.params - model.layer_freeze_regexes = None
2021-12-12 17:40:43,050 - INFO - allennlp.common.params - model.regularizer = None
2021-12-12 17:40:43,455 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748
2021-12-12 17:40:43,456 - INFO - transformers.configuration_utils - Model config RobertaConfig {
  "_num_labels": 2,
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "early_stopping": false,
  "eos_token_id": 2,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 514,
  "min_length": 0,
  "model_type": "roberta",
  "no_repeat_ngram_size": 0,
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 1,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

2021-12-12 17:40:43,895 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /vol/bitbucket/aeg19/.cache/torch/transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536
2021-12-12 17:40:54,997 - INFO - allennlp.common.params - data_loader.type = default
2021-12-12 17:40:54,998 - INFO - allennlp.common.params - data_loader.batch_size = 1
2021-12-12 17:40:54,998 - INFO - allennlp.common.params - data_loader.shuffle = False
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.sampler = None
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.num_workers = 0
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.pin_memory = False
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.drop_last = False
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.timeout = 0
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2021-12-12 17:40:54,999 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2021-12-12 17:40:55,000 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2021-12-12 17:40:55,000 - INFO - allennlp.common.params - data_loader.batch_sampler.type = basic
2021-12-12 17:40:55,000 - INFO - allennlp.common.params - data_loader.batch_sampler.sampler = random
2021-12-12 17:40:55,000 - INFO - allennlp.common.params - type = random
2021-12-12 17:40:55,001 - INFO - allennlp.common.params - replacement = False
2021-12-12 17:40:55,001 - INFO - allennlp.common.params - num_samples = None
2021-12-12 17:40:55,002 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2021-12-12 17:40:55,002 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2021-12-12 17:40:55,005 - INFO - allennlp.common.params - data_loader.type = default
2021-12-12 17:40:55,005 - INFO - allennlp.common.params - data_loader.batch_size = 1
2021-12-12 17:40:55,006 - INFO - allennlp.common.params - data_loader.shuffle = False
2021-12-12 17:40:55,006 - INFO - allennlp.common.params - data_loader.sampler = None
2021-12-12 17:40:55,006 - INFO - allennlp.common.params - data_loader.num_workers = 0
2021-12-12 17:40:55,006 - INFO - allennlp.common.params - data_loader.pin_memory = False
2021-12-12 17:40:55,006 - INFO - allennlp.common.params - data_loader.drop_last = False
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - data_loader.timeout = 0
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - data_loader.batch_sampler.type = basic
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - data_loader.batch_sampler.sampler = random
2021-12-12 17:40:55,007 - INFO - allennlp.common.params - type = random
2021-12-12 17:40:55,008 - INFO - allennlp.common.params - replacement = False
2021-12-12 17:40:55,008 - INFO - allennlp.common.params - num_samples = None
2021-12-12 17:40:55,008 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2021-12-12 17:40:55,008 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.type = default
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.batch_size = 1
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.shuffle = False
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.sampler = None
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.num_workers = 0
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.pin_memory = False
2021-12-12 17:40:55,009 - INFO - allennlp.common.params - data_loader.drop_last = False
2021-12-12 17:40:55,010 - INFO - allennlp.common.params - data_loader.timeout = 0
2021-12-12 17:40:55,010 - INFO - allennlp.common.params - data_loader.worker_init_fn = None
2021-12-12 17:40:55,010 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None
2021-12-12 17:40:55,010 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None
2021-12-12 17:40:55,010 - INFO - allennlp.common.params - data_loader.batch_sampler.type = basic
2021-12-12 17:40:55,010 - INFO - allennlp.common.params - data_loader.batch_sampler.sampler = random
2021-12-12 17:40:55,011 - INFO - allennlp.common.params - type = random
2021-12-12 17:40:55,011 - INFO - allennlp.common.params - replacement = False
2021-12-12 17:40:55,011 - INFO - allennlp.common.params - num_samples = None
2021-12-12 17:40:55,012 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 2
2021-12-12 17:40:55,012 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False
2021-12-12 17:40:55,012 - INFO - allennlp.common.params - trainer.type = gradient_descent
2021-12-12 17:40:55,012 - INFO - allennlp.common.params - trainer.patience = 2
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.validation_metric = +EM
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.num_epochs = 4
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.cuda_device = 3
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.grad_norm = None
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.grad_clipping = 1
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.distributed = None
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.save_best_model = True
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.world_size = 1
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 8
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.opt_level = None
2021-12-12 17:40:55,013 - INFO - allennlp.common.params - trainer.no_grad = None
2021-12-12 17:40:55,014 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2021-12-12 17:40:55,014 - INFO - allennlp.common.params - trainer.tensorboard_writer = None
2021-12-12 17:40:55,014 - INFO - allennlp.common.params - trainer.moving_average = None
2021-12-12 17:40:55,014 - INFO - allennlp.common.params - trainer.batch_callbacks = None
2021-12-12 17:40:55,014 - INFO - allennlp.common.params - trainer.epoch_callbacks = None
2021-12-12 17:41:00,396 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):
2021-12-12 17:41:00,397 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):
2021-12-12 17:41:00,397 - INFO - allennlp.common.util - _transformer_model.embeddings.word_embeddings.weight
2021-12-12 17:41:00,397 - INFO - allennlp.common.util - _transformer_model.embeddings.position_embeddings.weight
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.embeddings.token_type_embeddings.weight
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.embeddings.LayerNorm.weight
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.embeddings.LayerNorm.bias
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.self.query.weight
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.self.query.bias
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.self.key.weight
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.self.key.bias
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.self.value.weight
2021-12-12 17:41:00,398 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.self.value.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.output.dense.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.output.dense.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.output.LayerNorm.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.attention.output.LayerNorm.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.intermediate.dense.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.intermediate.dense.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.output.dense.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.output.dense.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.output.LayerNorm.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.0.output.LayerNorm.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.self.query.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.self.query.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.self.key.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.self.key.bias
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.self.value.weight
2021-12-12 17:41:00,399 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.self.value.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.output.dense.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.output.dense.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.output.LayerNorm.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.attention.output.LayerNorm.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.intermediate.dense.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.intermediate.dense.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.output.dense.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.output.dense.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.output.LayerNorm.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.1.output.LayerNorm.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.self.query.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.self.query.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.self.key.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.self.key.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.self.value.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.self.value.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.output.dense.weight
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.output.dense.bias
2021-12-12 17:41:00,400 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.output.LayerNorm.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.attention.output.LayerNorm.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.intermediate.dense.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.intermediate.dense.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.output.dense.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.output.dense.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.output.LayerNorm.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.2.output.LayerNorm.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.self.query.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.self.query.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.self.key.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.self.key.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.self.value.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.self.value.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.output.dense.weight
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.output.dense.bias
2021-12-12 17:41:00,401 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.output.LayerNorm.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.attention.output.LayerNorm.bias
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.intermediate.dense.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.intermediate.dense.bias
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.output.dense.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.output.dense.bias
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.output.LayerNorm.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.3.output.LayerNorm.bias
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.self.query.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.self.query.bias
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.self.key.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.self.key.bias
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.self.value.weight
2021-12-12 17:41:00,402 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.self.value.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.output.dense.weight
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.output.dense.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.output.LayerNorm.weight
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.attention.output.LayerNorm.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.intermediate.dense.weight
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.intermediate.dense.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.output.dense.weight
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.output.dense.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.output.LayerNorm.weight
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.4.output.LayerNorm.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.self.query.weight
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.self.query.bias
2021-12-12 17:41:00,403 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.self.key.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.self.key.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.self.value.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.self.value.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.output.dense.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.output.dense.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.output.LayerNorm.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.attention.output.LayerNorm.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.intermediate.dense.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.intermediate.dense.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.output.dense.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.output.dense.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.output.LayerNorm.weight
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.5.output.LayerNorm.bias
2021-12-12 17:41:00,404 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.self.query.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.self.query.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.self.key.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.self.key.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.self.value.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.self.value.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.output.dense.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.output.dense.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.output.LayerNorm.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.attention.output.LayerNorm.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.intermediate.dense.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.intermediate.dense.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.output.dense.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.output.dense.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.output.LayerNorm.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.6.output.LayerNorm.bias
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.self.query.weight
2021-12-12 17:41:00,405 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.self.query.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.self.key.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.self.key.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.self.value.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.self.value.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.output.dense.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.output.dense.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.output.LayerNorm.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.attention.output.LayerNorm.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.intermediate.dense.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.intermediate.dense.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.output.dense.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.output.dense.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.output.LayerNorm.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.7.output.LayerNorm.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.self.query.weight
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.self.query.bias
2021-12-12 17:41:00,406 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.self.key.weight
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.self.key.bias
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.self.value.weight
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.self.value.bias
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.output.dense.weight
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.output.dense.bias
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.output.LayerNorm.weight
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.attention.output.LayerNorm.bias
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.intermediate.dense.weight
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.intermediate.dense.bias
2021-12-12 17:41:00,407 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.output.dense.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.output.dense.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.output.LayerNorm.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.8.output.LayerNorm.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.self.query.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.self.query.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.self.key.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.self.key.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.self.value.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.self.value.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.output.dense.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.output.dense.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.output.LayerNorm.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.attention.output.LayerNorm.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.intermediate.dense.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.intermediate.dense.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.output.dense.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.output.dense.bias
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.output.LayerNorm.weight
2021-12-12 17:41:00,408 - INFO - allennlp.common.util - _transformer_model.encoder.layer.9.output.LayerNorm.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.self.query.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.self.query.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.self.key.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.self.key.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.self.value.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.self.value.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.output.dense.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.output.dense.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.output.LayerNorm.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.attention.output.LayerNorm.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.intermediate.dense.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.intermediate.dense.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.output.dense.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.output.dense.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.output.LayerNorm.weight
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.10.output.LayerNorm.bias
2021-12-12 17:41:00,409 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.self.query.weight
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.self.query.bias
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.self.key.weight
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.self.key.bias
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.self.value.weight
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.self.value.bias
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.output.dense.weight
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.output.dense.bias
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.output.LayerNorm.weight
2021-12-12 17:41:00,410 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.attention.output.LayerNorm.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.intermediate.dense.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.intermediate.dense.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.output.dense.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.output.dense.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.output.LayerNorm.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.11.output.LayerNorm.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.self.query.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.self.query.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.self.key.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.self.key.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.self.value.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.self.value.bias
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.output.dense.weight
2021-12-12 17:41:00,411 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.output.dense.bias
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.output.LayerNorm.weight
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.attention.output.LayerNorm.bias
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.intermediate.dense.weight
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.intermediate.dense.bias
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.output.dense.weight
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.output.dense.bias
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.output.LayerNorm.weight
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.12.output.LayerNorm.bias
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.self.query.weight
2021-12-12 17:41:00,412 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.self.query.bias
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.self.key.weight
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.self.key.bias
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.self.value.weight
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.self.value.bias
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.output.dense.weight
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.output.dense.bias
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.output.LayerNorm.weight
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.attention.output.LayerNorm.bias
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.intermediate.dense.weight
2021-12-12 17:41:00,413 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.intermediate.dense.bias
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.output.dense.weight
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.output.dense.bias
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.output.LayerNorm.weight
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.13.output.LayerNorm.bias
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.self.query.weight
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.self.query.bias
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.self.key.weight
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.self.key.bias
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.self.value.weight
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.self.value.bias
2021-12-12 17:41:00,414 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.output.dense.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.output.dense.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.output.LayerNorm.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.attention.output.LayerNorm.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.intermediate.dense.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.intermediate.dense.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.output.dense.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.output.dense.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.output.LayerNorm.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.14.output.LayerNorm.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.self.query.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.self.query.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.self.key.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.self.key.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.self.value.weight
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.self.value.bias
2021-12-12 17:41:00,415 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.output.dense.weight
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.output.dense.bias
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.output.LayerNorm.weight
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.attention.output.LayerNorm.bias
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.intermediate.dense.weight
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.intermediate.dense.bias
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.output.dense.weight
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.output.dense.bias
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.output.LayerNorm.weight
2021-12-12 17:41:00,416 - INFO - allennlp.common.util - _transformer_model.encoder.layer.15.output.LayerNorm.bias
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.self.query.weight
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.self.query.bias
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.self.key.weight
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.self.key.bias
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.self.value.weight
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.self.value.bias
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.output.dense.weight
2021-12-12 17:41:00,417 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.output.dense.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.output.LayerNorm.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.attention.output.LayerNorm.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.intermediate.dense.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.intermediate.dense.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.output.dense.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.output.dense.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.output.LayerNorm.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.16.output.LayerNorm.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.self.query.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.self.query.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.self.key.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.self.key.bias
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.self.value.weight
2021-12-12 17:41:00,418 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.self.value.bias
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.output.dense.weight
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.output.dense.bias
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.output.LayerNorm.weight
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.attention.output.LayerNorm.bias
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.intermediate.dense.weight
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.intermediate.dense.bias
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.output.dense.weight
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.output.dense.bias
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.output.LayerNorm.weight
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.17.output.LayerNorm.bias
2021-12-12 17:41:00,419 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.self.query.weight
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.self.query.bias
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.self.key.weight
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.self.key.bias
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.self.value.weight
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.self.value.bias
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.output.dense.weight
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.output.dense.bias
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.output.LayerNorm.weight
2021-12-12 17:41:00,420 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.attention.output.LayerNorm.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.intermediate.dense.weight
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.intermediate.dense.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.output.dense.weight
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.output.dense.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.output.LayerNorm.weight
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.18.output.LayerNorm.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.self.query.weight
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.self.query.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.self.key.weight
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.self.key.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.self.value.weight
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.self.value.bias
2021-12-12 17:41:00,421 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.output.dense.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.output.dense.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.output.LayerNorm.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.attention.output.LayerNorm.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.intermediate.dense.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.intermediate.dense.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.output.dense.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.output.dense.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.output.LayerNorm.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.19.output.LayerNorm.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.self.query.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.self.query.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.self.key.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.self.key.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.self.value.weight
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.self.value.bias
2021-12-12 17:41:00,422 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.output.dense.weight
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.output.dense.bias
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.output.LayerNorm.weight
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.attention.output.LayerNorm.bias
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.intermediate.dense.weight
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.intermediate.dense.bias
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.output.dense.weight
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.output.dense.bias
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.output.LayerNorm.weight
2021-12-12 17:41:00,423 - INFO - allennlp.common.util - _transformer_model.encoder.layer.20.output.LayerNorm.bias
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.self.query.weight
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.self.query.bias
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.self.key.weight
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.self.key.bias
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.self.value.weight
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.self.value.bias
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.output.dense.weight
2021-12-12 17:41:00,424 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.output.dense.bias
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.output.LayerNorm.weight
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.attention.output.LayerNorm.bias
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.intermediate.dense.weight
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.intermediate.dense.bias
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.output.dense.weight
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.output.dense.bias
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.output.LayerNorm.weight
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.21.output.LayerNorm.bias
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.self.query.weight
2021-12-12 17:41:00,425 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.self.query.bias
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.self.key.weight
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.self.key.bias
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.self.value.weight
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.self.value.bias
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.output.dense.weight
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.output.dense.bias
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.output.LayerNorm.weight
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.attention.output.LayerNorm.bias
2021-12-12 17:41:00,426 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.intermediate.dense.weight
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.intermediate.dense.bias
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.output.dense.weight
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.output.dense.bias
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.output.LayerNorm.weight
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.22.output.LayerNorm.bias
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.self.query.weight
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.self.query.bias
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.self.key.weight
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.self.key.bias
2021-12-12 17:41:00,427 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.self.value.weight
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.self.value.bias
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.output.dense.weight
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.output.dense.bias
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.output.LayerNorm.weight
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.attention.output.LayerNorm.bias
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.intermediate.dense.weight
2021-12-12 17:41:00,428 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.intermediate.dense.bias
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.output.dense.weight
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.output.dense.bias
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.output.LayerNorm.weight
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _transformer_model.encoder.layer.23.output.LayerNorm.bias
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _transformer_model.pooler.dense.weight
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _transformer_model.pooler.dense.bias
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _classifier.weight
2021-12-12 17:41:00,429 - INFO - allennlp.common.util - _classifier.bias
2021-12-12 17:41:00,431 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw
2021-12-12 17:41:00,432 - INFO - allennlp.common.params - trainer.optimizer.lr = 1e-05
2021-12-12 17:41:00,432 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.9, 0.98]
2021-12-12 17:41:00,432 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06
2021-12-12 17:41:00,432 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1
2021-12-12 17:41:00,432 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False
2021-12-12 17:41:00,434 - INFO - allennlp.training.optimizers - Done constructing parameter groups.
2021-12-12 17:41:00,434 - INFO - allennlp.training.optimizers - Group 0: ['_transformer_model.encoder.layer.23.output.dense.bias', '_transformer_model.encoder.layer.11.output.LayerNorm.weight', '_transformer_model.encoder.layer.10.intermediate.dense.bias', '_transformer_model.encoder.layer.8.intermediate.dense.bias', '_transformer_model.encoder.layer.22.attention.output.dense.bias', '_transformer_model.encoder.layer.7.attention.self.query.bias', '_transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.6.output.LayerNorm.weight', '_transformer_model.encoder.layer.21.output.LayerNorm.bias', '_transformer_model.encoder.layer.19.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.0.attention.self.value.bias', '_transformer_model.encoder.layer.15.output.dense.bias', '_transformer_model.encoder.layer.22.output.LayerNorm.bias', '_transformer_model.encoder.layer.1.intermediate.dense.bias', '_transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.11.output.dense.bias', '_transformer_model.encoder.layer.23.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.17.attention.self.query.bias', '_transformer_model.encoder.layer.15.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.3.attention.output.dense.bias', '_transformer_model.encoder.layer.8.output.LayerNorm.bias', '_transformer_model.encoder.layer.18.output.LayerNorm.bias', '_transformer_model.encoder.layer.4.attention.output.dense.bias', '_transformer_model.encoder.layer.15.attention.self.key.bias', '_transformer_model.encoder.layer.20.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.19.output.dense.bias', '_transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.13.attention.self.value.bias', '_transformer_model.encoder.layer.4.output.dense.bias', '_transformer_model.encoder.layer.3.output.LayerNorm.bias', '_transformer_model.encoder.layer.10.attention.self.value.bias', '_transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.12.attention.self.key.bias', '_transformer_model.encoder.layer.19.attention.self.value.bias', '_transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.18.attention.self.query.bias', '_transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.20.intermediate.dense.bias', '_transformer_model.encoder.layer.4.attention.self.query.bias', '_transformer_model.encoder.layer.22.attention.self.key.bias', '_transformer_model.encoder.layer.21.attention.self.key.bias', '_transformer_model.encoder.layer.23.attention.self.key.bias', '_transformer_model.encoder.layer.6.intermediate.dense.bias', '_transformer_model.encoder.layer.5.intermediate.dense.bias', '_transformer_model.encoder.layer.10.output.LayerNorm.weight', '_transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.5.output.LayerNorm.bias', '_transformer_model.encoder.layer.14.attention.self.key.bias', '_transformer_model.encoder.layer.3.attention.self.query.bias', '_transformer_model.encoder.layer.18.output.dense.bias', '_transformer_model.encoder.layer.22.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.6.attention.output.dense.bias', '_transformer_model.encoder.layer.5.output.dense.bias', '_transformer_model.encoder.layer.13.attention.output.dense.bias', '_transformer_model.encoder.layer.2.attention.output.dense.bias', '_transformer_model.encoder.layer.17.output.dense.bias', '_transformer_model.encoder.layer.16.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.0.output.LayerNorm.bias', '_transformer_model.encoder.layer.2.output.dense.bias', '_transformer_model.embeddings.LayerNorm.bias', '_transformer_model.encoder.layer.14.output.LayerNorm.bias', '_transformer_model.encoder.layer.19.intermediate.dense.bias', '_transformer_model.encoder.layer.1.output.LayerNorm.weight', '_transformer_model.encoder.layer.7.output.LayerNorm.weight', '_transformer_model.encoder.layer.6.attention.self.key.bias', '_transformer_model.encoder.layer.12.attention.self.query.bias', '_transformer_model.encoder.layer.12.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.9.attention.self.value.bias', '_transformer_model.encoder.layer.6.attention.self.value.bias', '_transformer_model.encoder.layer.19.attention.self.query.bias', '_transformer_model.encoder.layer.22.output.LayerNorm.weight', '_transformer_model.encoder.layer.8.output.LayerNorm.weight', '_transformer_model.encoder.layer.12.attention.self.value.bias', '_transformer_model.encoder.layer.19.attention.self.key.bias', '_transformer_model.encoder.layer.10.attention.self.key.bias', '_transformer_model.encoder.layer.17.attention.output.dense.bias', '_transformer_model.encoder.layer.14.attention.output.dense.bias', '_transformer_model.encoder.layer.10.attention.self.query.bias', '_transformer_model.encoder.layer.12.output.dense.bias', '_transformer_model.encoder.layer.14.attention.self.value.bias', '_transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.8.output.dense.bias', '_transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.13.output.LayerNorm.bias', '_transformer_model.encoder.layer.7.attention.self.key.bias', '_transformer_model.encoder.layer.17.attention.self.value.bias', '_transformer_model.encoder.layer.8.attention.self.query.bias', '_transformer_model.encoder.layer.10.output.dense.bias', '_transformer_model.encoder.layer.20.output.LayerNorm.weight', '_transformer_model.encoder.layer.7.intermediate.dense.bias', '_transformer_model.embeddings.LayerNorm.weight', '_transformer_model.encoder.layer.1.attention.self.query.bias', '_transformer_model.encoder.layer.18.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.17.intermediate.dense.bias', '_transformer_model.encoder.layer.5.output.LayerNorm.weight', '_transformer_model.encoder.layer.1.attention.self.key.bias', '_transformer_model.encoder.layer.16.intermediate.dense.bias', '_transformer_model.encoder.layer.4.attention.self.key.bias', '_transformer_model.encoder.layer.14.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.1.output.LayerNorm.bias', '_transformer_model.encoder.layer.12.attention.output.dense.bias', '_transformer_model.encoder.layer.9.output.LayerNorm.weight', '_transformer_model.encoder.layer.16.output.LayerNorm.bias', '_transformer_model.encoder.layer.9.attention.self.key.bias', '_transformer_model.encoder.layer.13.attention.self.query.bias', '_transformer_model.encoder.layer.6.output.dense.bias', '_transformer_model.encoder.layer.16.attention.self.query.bias', '_transformer_model.encoder.layer.1.attention.self.value.bias', '_transformer_model.encoder.layer.20.attention.self.value.bias', '_transformer_model.encoder.layer.16.output.LayerNorm.weight', '_transformer_model.encoder.layer.20.attention.self.query.bias', '_transformer_model.encoder.layer.21.attention.self.query.bias', '_transformer_model.pooler.dense.bias', '_transformer_model.encoder.layer.22.attention.self.value.bias', '_transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.13.intermediate.dense.bias', '_transformer_model.encoder.layer.22.output.dense.bias', '_transformer_model.encoder.layer.9.output.LayerNorm.bias', '_transformer_model.encoder.layer.18.attention.self.key.bias', '_transformer_model.encoder.layer.11.attention.self.query.bias', '_transformer_model.encoder.layer.23.attention.output.dense.bias', '_transformer_model.encoder.layer.20.attention.self.key.bias', '_transformer_model.encoder.layer.3.output.LayerNorm.weight', '_transformer_model.encoder.layer.11.intermediate.dense.bias', '_transformer_model.encoder.layer.4.intermediate.dense.bias', '_transformer_model.encoder.layer.16.attention.self.key.bias', '_transformer_model.encoder.layer.18.attention.output.dense.bias', '_transformer_model.encoder.layer.19.attention.output.dense.bias', '_transformer_model.encoder.layer.21.intermediate.dense.bias', '_transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.4.output.LayerNorm.bias', '_transformer_model.encoder.layer.23.attention.self.query.bias', '_transformer_model.encoder.layer.2.intermediate.dense.bias', '_transformer_model.encoder.layer.21.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.12.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.0.attention.self.query.bias', '_transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.17.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.16.output.dense.bias', '_transformer_model.encoder.layer.13.output.LayerNorm.weight', '_transformer_model.encoder.layer.22.attention.self.query.bias', '_transformer_model.encoder.layer.20.output.LayerNorm.bias', '_transformer_model.encoder.layer.8.attention.self.value.bias', '_transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.23.output.LayerNorm.bias', '_transformer_model.encoder.layer.21.output.LayerNorm.weight', '_transformer_model.encoder.layer.8.attention.output.dense.bias', '_transformer_model.encoder.layer.3.output.dense.bias', '_transformer_model.encoder.layer.13.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.0.output.dense.bias', '_transformer_model.encoder.layer.2.attention.self.value.bias', '_transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.9.attention.self.query.bias', '_transformer_model.encoder.layer.14.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.5.attention.output.dense.bias', '_transformer_model.encoder.layer.17.attention.self.key.bias', '_transformer_model.encoder.layer.11.attention.output.dense.bias', '_transformer_model.encoder.layer.8.attention.self.key.bias', '_transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.21.output.dense.bias', '_transformer_model.encoder.layer.14.output.LayerNorm.weight', '_transformer_model.encoder.layer.4.output.LayerNorm.weight', '_transformer_model.encoder.layer.5.attention.self.key.bias', '_transformer_model.encoder.layer.19.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.2.output.LayerNorm.weight', '_transformer_model.encoder.layer.19.output.LayerNorm.weight', '_transformer_model.encoder.layer.18.intermediate.dense.bias', '_transformer_model.encoder.layer.23.attention.self.value.bias', '_transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.18.attention.self.value.bias', '_classifier.bias', '_transformer_model.encoder.layer.16.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.15.intermediate.dense.bias', '_transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.22.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.23.output.LayerNorm.weight', '_transformer_model.encoder.layer.2.attention.self.query.bias', '_transformer_model.encoder.layer.6.attention.self.query.bias', '_transformer_model.encoder.layer.9.output.dense.bias', '_transformer_model.encoder.layer.18.output.LayerNorm.weight', '_transformer_model.encoder.layer.14.output.dense.bias', '_transformer_model.encoder.layer.0.attention.self.key.bias', '_transformer_model.encoder.layer.3.attention.self.key.bias', '_transformer_model.encoder.layer.23.intermediate.dense.bias', '_transformer_model.encoder.layer.0.output.LayerNorm.weight', '_transformer_model.encoder.layer.12.output.LayerNorm.weight', '_transformer_model.encoder.layer.1.output.dense.bias', '_transformer_model.encoder.layer.4.attention.self.value.bias', '_transformer_model.encoder.layer.12.intermediate.dense.bias', '_transformer_model.encoder.layer.17.output.LayerNorm.weight', '_transformer_model.encoder.layer.19.output.LayerNorm.bias', '_transformer_model.encoder.layer.2.attention.self.key.bias', '_transformer_model.encoder.layer.7.attention.self.value.bias', '_transformer_model.encoder.layer.2.output.LayerNorm.bias', '_transformer_model.encoder.layer.3.intermediate.dense.bias', '_transformer_model.encoder.layer.7.output.LayerNorm.bias', '_transformer_model.encoder.layer.6.output.LayerNorm.bias', '_transformer_model.encoder.layer.15.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.21.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.5.attention.self.value.bias', '_transformer_model.encoder.layer.14.intermediate.dense.bias', '_transformer_model.encoder.layer.7.attention.output.dense.bias', '_transformer_model.encoder.layer.11.attention.self.key.bias', '_transformer_model.encoder.layer.12.output.LayerNorm.bias', '_transformer_model.encoder.layer.21.attention.self.value.bias', '_transformer_model.encoder.layer.20.attention.output.dense.bias', '_transformer_model.encoder.layer.11.attention.self.value.bias', '_transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.0.intermediate.dense.bias', '_transformer_model.encoder.layer.14.attention.self.query.bias', '_transformer_model.encoder.layer.13.output.dense.bias', '_transformer_model.encoder.layer.1.attention.output.dense.bias', '_transformer_model.encoder.layer.16.attention.output.dense.bias', '_transformer_model.encoder.layer.15.output.LayerNorm.bias', '_transformer_model.encoder.layer.17.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.10.attention.output.dense.bias', '_transformer_model.encoder.layer.7.output.dense.bias', '_transformer_model.encoder.layer.3.attention.self.value.bias', '_transformer_model.encoder.layer.5.attention.self.query.bias', '_transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.9.intermediate.dense.bias', '_transformer_model.encoder.layer.13.attention.self.key.bias', '_transformer_model.encoder.layer.23.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.15.attention.self.value.bias', '_transformer_model.encoder.layer.10.output.LayerNorm.bias', '_transformer_model.encoder.layer.9.attention.output.dense.bias', '_transformer_model.encoder.layer.21.attention.output.dense.bias', '_transformer_model.encoder.layer.13.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.11.output.LayerNorm.bias', '_transformer_model.encoder.layer.15.output.LayerNorm.weight', '_transformer_model.encoder.layer.22.intermediate.dense.bias', '_transformer_model.encoder.layer.15.attention.output.dense.bias', '_transformer_model.encoder.layer.20.attention.output.LayerNorm.bias', '_transformer_model.encoder.layer.17.output.LayerNorm.bias', '_transformer_model.encoder.layer.16.attention.self.value.bias', '_transformer_model.encoder.layer.18.attention.output.LayerNorm.weight', '_transformer_model.encoder.layer.0.attention.output.dense.bias', '_transformer_model.encoder.layer.20.output.dense.bias', '_transformer_model.encoder.layer.15.attention.self.query.bias'], {'weight_decay': 0}
2021-12-12 17:41:00,434 - INFO - allennlp.training.optimizers - Group 1: ['_transformer_model.encoder.layer.18.attention.self.query.weight', '_transformer_model.encoder.layer.19.attention.self.key.weight', '_transformer_model.encoder.layer.7.intermediate.dense.weight', '_transformer_model.encoder.layer.2.attention.output.dense.weight', '_transformer_model.encoder.layer.13.attention.self.key.weight', '_transformer_model.encoder.layer.7.attention.self.key.weight', '_transformer_model.encoder.layer.8.output.dense.weight', '_transformer_model.encoder.layer.21.attention.self.value.weight', '_transformer_model.encoder.layer.21.attention.self.query.weight', '_transformer_model.encoder.layer.8.attention.self.query.weight', '_transformer_model.encoder.layer.23.output.dense.weight', '_transformer_model.encoder.layer.9.attention.self.value.weight', '_transformer_model.embeddings.position_embeddings.weight', '_transformer_model.encoder.layer.15.attention.self.query.weight', '_transformer_model.encoder.layer.6.attention.output.dense.weight', '_transformer_model.encoder.layer.9.attention.output.dense.weight', '_transformer_model.encoder.layer.9.intermediate.dense.weight', '_transformer_model.encoder.layer.5.output.dense.weight', '_transformer_model.encoder.layer.15.attention.self.value.weight', '_transformer_model.encoder.layer.12.intermediate.dense.weight', '_transformer_model.encoder.layer.16.output.dense.weight', '_transformer_model.encoder.layer.12.attention.output.dense.weight', '_transformer_model.encoder.layer.10.attention.self.query.weight', '_transformer_model.encoder.layer.3.attention.output.dense.weight', '_transformer_model.encoder.layer.20.intermediate.dense.weight', '_transformer_model.encoder.layer.2.intermediate.dense.weight', '_transformer_model.encoder.layer.11.attention.self.value.weight', '_transformer_model.encoder.layer.10.attention.self.value.weight', '_transformer_model.encoder.layer.22.attention.self.value.weight', '_transformer_model.encoder.layer.17.attention.self.key.weight', '_transformer_model.encoder.layer.1.attention.self.key.weight', '_transformer_model.encoder.layer.3.output.dense.weight', '_transformer_model.encoder.layer.19.attention.output.dense.weight', '_transformer_model.encoder.layer.21.output.dense.weight', '_transformer_model.encoder.layer.2.attention.self.value.weight', '_transformer_model.encoder.layer.16.attention.self.value.weight', '_transformer_model.encoder.layer.20.attention.self.query.weight', '_transformer_model.encoder.layer.15.attention.self.key.weight', '_transformer_model.encoder.layer.4.attention.output.dense.weight', '_transformer_model.encoder.layer.20.output.dense.weight', '_transformer_model.encoder.layer.3.intermediate.dense.weight', '_transformer_model.encoder.layer.16.intermediate.dense.weight', '_transformer_model.encoder.layer.0.attention.self.query.weight', '_transformer_model.encoder.layer.9.attention.self.key.weight', '_transformer_model.encoder.layer.10.output.dense.weight', '_transformer_model.encoder.layer.17.attention.self.value.weight', '_transformer_model.encoder.layer.5.attention.output.dense.weight', '_transformer_model.encoder.layer.16.attention.output.dense.weight', '_transformer_model.encoder.layer.3.attention.self.query.weight', '_transformer_model.embeddings.word_embeddings.weight', '_transformer_model.encoder.layer.18.intermediate.dense.weight', '_transformer_model.encoder.layer.11.attention.output.dense.weight', '_transformer_model.encoder.layer.15.attention.output.dense.weight', '_transformer_model.encoder.layer.15.output.dense.weight', '_transformer_model.encoder.layer.21.attention.output.dense.weight', '_transformer_model.encoder.layer.9.output.dense.weight', '_transformer_model.encoder.layer.11.output.dense.weight', '_transformer_model.encoder.layer.19.attention.self.query.weight', '_transformer_model.encoder.layer.6.attention.self.value.weight', '_transformer_model.encoder.layer.2.attention.self.query.weight', '_transformer_model.encoder.layer.21.attention.self.key.weight', '_transformer_model.encoder.layer.0.output.dense.weight', '_transformer_model.encoder.layer.17.output.dense.weight', '_transformer_model.encoder.layer.7.output.dense.weight', '_transformer_model.encoder.layer.19.output.dense.weight', '_transformer_model.encoder.layer.13.output.dense.weight', '_transformer_model.encoder.layer.20.attention.output.dense.weight', '_transformer_model.encoder.layer.1.output.dense.weight', '_transformer_model.encoder.layer.7.attention.self.query.weight', '_transformer_model.encoder.layer.12.output.dense.weight', '_transformer_model.encoder.layer.23.attention.self.value.weight', '_transformer_model.encoder.layer.17.attention.self.query.weight', '_transformer_model.encoder.layer.3.attention.self.key.weight', '_transformer_model.encoder.layer.4.output.dense.weight', '_transformer_model.encoder.layer.8.attention.self.key.weight', '_transformer_model.encoder.layer.14.attention.self.value.weight', '_transformer_model.encoder.layer.18.attention.self.key.weight', '_transformer_model.encoder.layer.14.attention.self.key.weight', '_transformer_model.encoder.layer.4.attention.self.value.weight', '_transformer_model.encoder.layer.20.attention.self.key.weight', '_transformer_model.embeddings.token_type_embeddings.weight', '_transformer_model.encoder.layer.14.attention.self.query.weight', '_transformer_model.encoder.layer.13.intermediate.dense.weight', '_transformer_model.encoder.layer.18.attention.output.dense.weight', '_transformer_model.encoder.layer.20.attention.self.value.weight', '_transformer_model.encoder.layer.22.attention.self.query.weight', '_transformer_model.encoder.layer.6.attention.self.key.weight', '_transformer_model.encoder.layer.8.intermediate.dense.weight', '_transformer_model.encoder.layer.23.intermediate.dense.weight', '_transformer_model.encoder.layer.22.attention.self.key.weight', '_transformer_model.encoder.layer.0.attention.self.key.weight', '_transformer_model.encoder.layer.4.attention.self.query.weight', '_transformer_model.encoder.layer.2.attention.self.key.weight', '_transformer_model.encoder.layer.23.attention.output.dense.weight', '_transformer_model.encoder.layer.6.intermediate.dense.weight', '_transformer_model.encoder.layer.6.output.dense.weight', '_transformer_model.encoder.layer.12.attention.self.query.weight', '_transformer_model.encoder.layer.10.attention.output.dense.weight', '_transformer_model.encoder.layer.18.output.dense.weight', '_transformer_model.pooler.dense.weight', '_transformer_model.encoder.layer.23.attention.self.key.weight', '_transformer_model.encoder.layer.7.attention.output.dense.weight', '_transformer_model.encoder.layer.17.intermediate.dense.weight', '_transformer_model.encoder.layer.4.attention.self.key.weight', '_transformer_model.encoder.layer.22.output.dense.weight', '_transformer_model.encoder.layer.13.attention.self.value.weight', '_transformer_model.encoder.layer.1.attention.self.value.weight', '_transformer_model.encoder.layer.13.attention.self.query.weight', '_transformer_model.encoder.layer.5.attention.self.value.weight', '_transformer_model.encoder.layer.7.attention.self.value.weight', '_transformer_model.encoder.layer.12.attention.self.value.weight', '_transformer_model.encoder.layer.1.attention.self.query.weight', '_transformer_model.encoder.layer.3.attention.self.value.weight', '_transformer_model.encoder.layer.5.attention.self.query.weight', '_transformer_model.encoder.layer.10.attention.self.key.weight', '_transformer_model.encoder.layer.0.intermediate.dense.weight', '_transformer_model.encoder.layer.0.attention.self.value.weight', '_transformer_model.encoder.layer.11.attention.self.key.weight', '_transformer_model.encoder.layer.14.attention.output.dense.weight', '_transformer_model.encoder.layer.16.attention.self.key.weight', '_transformer_model.encoder.layer.10.intermediate.dense.weight', '_transformer_model.encoder.layer.1.intermediate.dense.weight', '_transformer_model.encoder.layer.2.output.dense.weight', '_transformer_model.encoder.layer.16.attention.self.query.weight', '_transformer_model.encoder.layer.17.attention.output.dense.weight', '_transformer_model.encoder.layer.23.attention.self.query.weight', '_transformer_model.encoder.layer.11.intermediate.dense.weight', '_transformer_model.encoder.layer.18.attention.self.value.weight', '_transformer_model.encoder.layer.6.attention.self.query.weight', '_transformer_model.encoder.layer.22.attention.output.dense.weight', '_transformer_model.encoder.layer.12.attention.self.key.weight', '_transformer_model.encoder.layer.4.intermediate.dense.weight', '_transformer_model.encoder.layer.0.attention.output.dense.weight', '_transformer_model.encoder.layer.13.attention.output.dense.weight', '_transformer_model.encoder.layer.15.intermediate.dense.weight', '_transformer_model.encoder.layer.8.attention.self.value.weight', '_transformer_model.encoder.layer.21.intermediate.dense.weight', '_transformer_model.encoder.layer.14.intermediate.dense.weight', '_transformer_model.encoder.layer.5.intermediate.dense.weight', '_transformer_model.encoder.layer.5.attention.self.key.weight', '_transformer_model.encoder.layer.19.attention.self.value.weight', '_classifier.weight', '_transformer_model.encoder.layer.1.attention.output.dense.weight', '_transformer_model.encoder.layer.22.intermediate.dense.weight', '_transformer_model.encoder.layer.19.intermediate.dense.weight', '_transformer_model.encoder.layer.8.attention.output.dense.weight', '_transformer_model.encoder.layer.9.attention.self.query.weight', '_transformer_model.encoder.layer.11.attention.self.query.weight', '_transformer_model.encoder.layer.14.output.dense.weight'], {}
2021-12-12 17:41:00,435 - WARNING - allennlp.training.optimizers - When constructing parameter groups, layer_norm\.weight does not match any parameter name
2021-12-12 17:41:00,435 - INFO - allennlp.training.optimizers - Number of trainable parameters: 355361794
2021-12-12 17:41:00,439 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular
2021-12-12 17:41:00,439 - WARNING - allennlp.common.from_params - Parameter num_epochs for class SlantedTriangular was found in both **extras and in params. Using the specification found in params, but you probably put a key in a config file that you didn't need, and if it is different from what we get from **extras, you might get unexpected behavior.
2021-12-12 17:41:00,439 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_epochs = 4
2021-12-12 17:41:00,439 - WARNING - allennlp.common.from_params - Parameter num_steps_per_epoch for class SlantedTriangular was found in both **extras and in params. Using the specification found in params, but you probably put a key in a config file that you didn't need, and if it is different from what we get from **extras, you might get unexpected behavior.
2021-12-12 17:41:00,440 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.num_steps_per_epoch = 7442
2021-12-12 17:41:00,440 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06
2021-12-12 17:41:00,440 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32
2021-12-12 17:41:00,440 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1
2021-12-12 17:41:00,440 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False
2021-12-12 17:41:00,440 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False
2021-12-12 17:41:00,441 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38
2021-12-12 17:41:00,441 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None
2021-12-12 17:41:00,441 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = 1
2021-12-12 17:41:00,441 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None
2021-12-12 17:41:00,465 - INFO - allennlp.training.trainer - Beginning training.
2021-12-12 17:41:00,465 - INFO - allennlp.training.trainer - Epoch 0/3
2021-12-12 17:41:00,465 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 9889.948
2021-12-12 17:41:01,139 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 3
2021-12-12 17:41:01,140 - INFO - allennlp.training.trainer - GPU 1 memory usage MB: 2540
2021-12-12 17:41:01,150 - INFO - allennlp.training.trainer - GPU 2 memory usage MB: 3
2021-12-12 17:41:01,161 - INFO - allennlp.training.trainer - GPU 3 memory usage MB: 3
2021-12-12 17:41:01,171 - INFO - allennlp.training.trainer - GPU 4 memory usage MB: 29841
2021-12-12 17:41:01,197 - INFO - allennlp.training.trainer - GPU 5 memory usage MB: 3
2021-12-12 17:41:01,202 - INFO - allennlp.training.trainer - GPU 6 memory usage MB: 3
2021-12-12 17:41:01,213 - INFO - allennlp.training.trainer - GPU 7 memory usage MB: 3
2021-12-12 17:41:01,240 - INFO - allennlp.training.trainer - GPU 8 memory usage MB: 3
2021-12-12 17:41:01,245 - INFO - allennlp.training.trainer - GPU 9 memory usage MB: 22738
2021-12-12 17:41:01,267 - INFO - allennlp.training.trainer - Training

  0%|                                                                                                                                  | 0/4361 [00:00<?, ?it/s]/vol/bitbucket/aeg19/.envs/lorikeet/leapofthought/lib/python3.6/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
EM: 0.7500, loss: 0.5903, reg_loss: 0.0000 ||:   0%|                                                                         | 1/4361 [00:02<2:35:56,  2.15s/it]EM: 0.6875, loss: 0.6330, reg_loss: 0.0000 ||:   0%|                                                                         | 2/4361 [00:03<2:01:11,  1.67s/it]EM: 0.5833, loss: 0.6777, reg_loss: 0.0000 ||:   0%|                                                                         | 3/4361 [00:04<1:49:19,  1.51s/it]EM: 0.5938, loss: 0.6760, reg_loss: 0.0000 ||:   0%|                                                                         | 4/4361 [00:06<1:41:49,  1.40s/it]EM: 0.5875, loss: 0.6785, reg_loss: 0.0000 ||:   0%|                                                                         | 5/4361 [00:07<1:36:49,  1.33s/it]EM: 0.5938, loss: 0.6725, reg_loss: 0.0000 ||:   0%|                                                                         | 6/4361 [00:08<1:36:12,  1.33s/it]EM: 0.5893, loss: 0.6670, reg_loss: 0.0000 ||:   0%|                                                                         | 7/4361 [00:09<1:34:08,  1.30s/it]EM: 0.5781, loss: 0.6734, reg_loss: 0.0000 ||:   0%|‚ñè                                                                        | 8/4361 [00:11<1:38:35,  1.36s/it]EM: 0.5694, loss: 0.6761, reg_loss: 0.0000 ||:   0%|‚ñè                                                                        | 9/4361 [00:12<1:34:22,  1.30s/it]EM: 0.5813, loss: 0.6735, reg_loss: 0.0000 ||:   0%|‚ñè                                                                       | 10/4361 [00:14<1:40:05,  1.38s/it]EM: 0.5511, loss: 0.6848, reg_loss: 0.0000 ||:   0%|‚ñè                                                                       | 11/4361 [00:15<1:36:58,  1.34s/it]EM: 0.5521, loss: 0.6876, reg_loss: 0.0000 ||:   0%|‚ñè                                                                       | 12/4361 [00:16<1:35:16,  1.31s/it]EM: 0.5529, loss: 0.6875, reg_loss: 0.0000 ||:   0%|‚ñè                                                                       | 13/4361 [00:17<1:33:06,  1.28s/it]EM: 0.5580, loss: 0.6854, reg_loss: 0.0000 ||:   0%|‚ñè                                                                       | 14/4361 [00:18<1:30:16,  1.25s/it]EM: 0.5667, loss: 0.6825, reg_loss: 0.0000 ||:   0%|‚ñè                                                                       | 15/4361 [00:20<1:36:09,  1.33s/it]EM: 0.5625, loss: 0.6835, reg_loss: 0.0000 ||:   0%|‚ñé                                                                       | 16/4361 [00:21<1:36:48,  1.34s/it]EM: 0.5551, loss: 0.6850, reg_loss: 0.0000 ||:   0%|‚ñé                                                                       | 17/4361 [00:23<1:34:46,  1.31s/it]EM: 0.5451, loss: 0.6878, reg_loss: 0.0000 ||:   0%|‚ñé                                                                       | 18/4361 [00:24<1:36:08,  1.33s/it]EM: 0.5526, loss: 0.6859, reg_loss: 0.0000 ||:   0%|‚ñé                                                                       | 19/4361 [00:25<1:35:03,  1.31s/it]EM: 0.5437, loss: 0.6859, reg_loss: 0.0000 ||:   0%|‚ñé                                                                       | 20/4361 [00:26<1:33:42,  1.30s/it]EM: 0.5327, loss: 0.6901, reg_loss: 0.0000 ||:   0%|‚ñé                                                                       | 21/4361 [00:28<1:32:17,  1.28s/it]EM: 0.5284, loss: 0.6933, reg_loss: 0.0000 ||:   1%|‚ñé                                                                       | 22/4361 [00:29<1:32:02,  1.27s/it]EM: 0.5299, loss: 0.6925, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 23/4361 [00:30<1:31:46,  1.27s/it]EM: 0.5312, loss: 0.6935, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 24/4361 [00:32<1:33:32,  1.29s/it]EM: 0.5225, loss: 0.6948, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 25/4361 [00:33<1:33:28,  1.29s/it]EM: 0.5240, loss: 0.6945, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 26/4361 [00:34<1:30:50,  1.26s/it]EM: 0.5278, loss: 0.6929, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 27/4361 [00:35<1:30:15,  1.25s/it]EM: 0.5312, loss: 0.6925, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 28/4361 [00:36<1:30:15,  1.25s/it]EM: 0.5302, loss: 0.6929, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 29/4361 [00:38<1:28:00,  1.22s/it]EM: 0.5312, loss: 0.6929, reg_loss: 0.0000 ||:   1%|‚ñç                                                                       | 30/4361 [00:39<1:27:49,  1.22s/it]EM: 0.5323, loss: 0.6940, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 31/4361 [00:40<1:28:05,  1.22s/it]EM: 0.5312, loss: 0.6949, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 32/4361 [00:41<1:24:26,  1.17s/it]EM: 0.5360, loss: 0.6934, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 33/4361 [00:42<1:23:44,  1.16s/it]EM: 0.5312, loss: 0.6941, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 34/4361 [00:43<1:24:27,  1.17s/it]EM: 0.5339, loss: 0.6949, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 35/4361 [00:45<1:26:36,  1.20s/it]EM: 0.5347, loss: 0.6945, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 36/4361 [00:46<1:28:37,  1.23s/it]EM: 0.5304, loss: 0.6943, reg_loss: 0.0000 ||:   1%|‚ñå                                                                       | 37/4361 [00:47<1:29:55,  1.25s/it]EM: 0.5312, loss: 0.6947, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 38/4361 [00:49<1:30:33,  1.26s/it]EM: 0.5304, loss: 0.6959, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 39/4361 [00:50<1:30:17,  1.25s/it]EM: 0.5344, loss: 0.6946, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 40/4361 [00:51<1:28:58,  1.24s/it]EM: 0.5290, loss: 0.6963, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 41/4361 [00:52<1:29:52,  1.25s/it]EM: 0.5268, loss: 0.6970, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 42/4361 [00:54<1:28:47,  1.23s/it]EM: 0.5276, loss: 0.6978, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 43/4361 [00:55<1:32:10,  1.28s/it]EM: 0.5227, loss: 0.6982, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 44/4361 [00:56<1:32:12,  1.28s/it]EM: 0.5236, loss: 0.6983, reg_loss: 0.0000 ||:   1%|‚ñã                                                                       | 45/4361 [00:57<1:32:47,  1.29s/it]EM: 0.5245, loss: 0.6980, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 46/4361 [00:59<1:31:43,  1.28s/it]EM: 0.5239, loss: 0.6977, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 47/4361 [01:00<1:31:40,  1.28s/it]EM: 0.5234, loss: 0.6979, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 48/4361 [01:01<1:31:00,  1.27s/it]EM: 0.5255, loss: 0.6977, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 49/4361 [01:03<1:31:32,  1.27s/it]EM: 0.5225, loss: 0.6981, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 50/4361 [01:04<1:31:06,  1.27s/it]EM: 0.5208, loss: 0.6984, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 51/4361 [01:05<1:29:03,  1.24s/it]EM: 0.5192, loss: 0.6983, reg_loss: 0.0000 ||:   1%|‚ñä                                                                       | 52/4361 [01:06<1:28:22,  1.23s/it]EM: 0.5224, loss: 0.6973, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 53/4361 [01:07<1:30:01,  1.25s/it]EM: 0.5197, loss: 0.6979, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 54/4361 [01:09<1:30:54,  1.27s/it]EM: 0.5205, loss: 0.6977, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 55/4361 [01:10<1:30:03,  1.25s/it]EM: 0.5212, loss: 0.6975, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 56/4361 [01:11<1:29:47,  1.25s/it]EM: 0.5208, loss: 0.6981, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 57/4361 [01:12<1:28:58,  1.24s/it]EM: 0.5216, loss: 0.6977, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 58/4361 [01:14<1:28:19,  1.23s/it]EM: 0.5180, loss: 0.6984, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 59/4361 [01:15<1:26:58,  1.21s/it]EM: 0.5135, loss: 0.6995, reg_loss: 0.0000 ||:   1%|‚ñâ                                                                       | 60/4361 [01:16<1:28:56,  1.24s/it]EM: 0.5154, loss: 0.6993, reg_loss: 0.0000 ||:   1%|‚ñà                                                                       | 61/4361 [01:17<1:26:49,  1.21s/it]EM: 0.5161, loss: 0.6991, reg_loss: 0.0000 ||:   1%|‚ñà                                                                       | 62/4361 [01:18<1:26:39,  1.21s/it]EM: 0.5159, loss: 0.6986, reg_loss: 0.0000 ||:   1%|‚ñà                                                                       | 63/4361 [01:20<1:26:45,  1.21s/it]EM: 0.5146, loss: 0.6990, reg_loss: 0.0000 ||:   1%|‚ñà                                                                       | 64/4361 [01:21<1:27:32,  1.22s/it]EM: 0.5144, loss: 0.6992, reg_loss: 0.0000 ||:   1%|‚ñà                                                                       | 65/4361 [01:22<1:27:21,  1.22s/it]EM: 0.5123, loss: 0.6995, reg_loss: 0.0000 ||:   2%|‚ñà                                                                       | 66/4361 [01:23<1:25:41,  1.20s/it]EM: 0.5131, loss: 0.6996, reg_loss: 0.0000 ||:   2%|‚ñà                                                                       | 67/4361 [01:24<1:23:37,  1.17s/it]EM: 0.5119, loss: 0.6995, reg_loss: 0.0000 ||:   2%|‚ñà                                                                       | 68/4361 [01:26<1:26:28,  1.21s/it]EM: 0.5118, loss: 0.6996, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 69/4361 [01:27<1:27:37,  1.22s/it]EM: 0.5089, loss: 0.7000, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 70/4361 [01:28<1:31:20,  1.28s/it]EM: 0.5079, loss: 0.7004, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 71/4361 [01:30<1:30:32,  1.27s/it]EM: 0.5095, loss: 0.7000, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 72/4361 [01:31<1:30:15,  1.26s/it]EM: 0.5128, loss: 0.6993, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 73/4361 [01:32<1:31:02,  1.27s/it]EM: 0.5093, loss: 0.7001, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 74/4361 [01:34<1:32:20,  1.29s/it]EM: 0.5125, loss: 0.6995, reg_loss: 0.0000 ||:   2%|‚ñà‚ñè                                                                      | 75/4361 [01:35<1:31:20,  1.28s/it]EM: 0.5107, loss: 0.6997, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 76/4361 [01:36<1:29:34,  1.25s/it]EM: 0.5106, loss: 0.6998, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 77/4361 [01:37<1:31:24,  1.28s/it]EM: 0.5104, loss: 0.6998, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 78/4361 [01:39<1:31:37,  1.28s/it]EM: 0.5111, loss: 0.6996, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 79/4361 [01:40<1:30:59,  1.27s/it]EM: 0.5109, loss: 0.6998, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 80/4361 [01:41<1:29:20,  1.25s/it]EM: 0.5093, loss: 0.6996, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 81/4361 [01:42<1:27:18,  1.22s/it]EM: 0.5084, loss: 0.6999, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 82/4361 [01:43<1:26:28,  1.21s/it]EM: 0.5075, loss: 0.7001, reg_loss: 0.0000 ||:   2%|‚ñà‚ñé                                                                      | 83/4361 [01:45<1:25:19,  1.20s/it]EM: 0.5067, loss: 0.7002, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 84/4361 [01:46<1:24:07,  1.18s/it]EM: 0.5066, loss: 0.6996, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 85/4361 [01:47<1:24:38,  1.19s/it]EM: 0.5073, loss: 0.6997, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 86/4361 [01:48<1:23:46,  1.18s/it]EM: 0.5057, loss: 0.7001, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 87/4361 [01:49<1:26:54,  1.22s/it]EM: 0.5085, loss: 0.6998, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 88/4361 [01:51<1:27:35,  1.23s/it]EM: 0.5084, loss: 0.6996, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 89/4361 [01:52<1:26:30,  1.22s/it]EM: 0.5083, loss: 0.6997, reg_loss: 0.0000 ||:   2%|‚ñà‚ñç                                                                      | 90/4361 [01:53<1:23:17,  1.17s/it]EM: 0.5069, loss: 0.6997, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 91/4361 [01:54<1:24:46,  1.19s/it]EM: 0.5048, loss: 0.7002, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 92/4361 [01:55<1:26:35,  1.22s/it]EM: 0.5060, loss: 0.6998, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 93/4361 [01:57<1:24:48,  1.19s/it]EM: 0.5060, loss: 0.6997, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 94/4361 [01:58<1:24:46,  1.19s/it]EM: 0.5046, loss: 0.6998, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 95/4361 [01:59<1:25:45,  1.21s/it]EM: 0.5065, loss: 0.6993, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 96/4361 [02:00<1:22:04,  1.15s/it]EM: 0.5071, loss: 0.6993, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 97/4361 [02:01<1:22:44,  1.16s/it]EM: 0.5089, loss: 0.6990, reg_loss: 0.0000 ||:   2%|‚ñà‚ñå                                                                      | 98/4361 [02:02<1:25:39,  1.21s/it]EM: 0.5088, loss: 0.6990, reg_loss: 0.0000 ||:   2%|‚ñà‚ñã                                                                      | 99/4361 [02:04<1:24:46,  1.19s/it]EM: 0.5094, loss: 0.6989, reg_loss: 0.0000 ||:   2%|‚ñà‚ñã                                                                     | 100/4361 [02:06<1:46:45,  1.50s/it]EM: 0.5087, loss: 0.6992, reg_loss: 0.0000 ||:   2%|‚ñà‚ñã                                                                     | 101/4361 [02:07<1:46:25,  1.50s/it]EM: 0.5074, loss: 0.6994, reg_loss: 0.0000 ||:   2%|‚ñà‚ñã                                                                     | 102/4361 [02:09<1:40:59,  1.42s/it]